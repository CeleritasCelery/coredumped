#+STARTUP: content
#+AUTHOR: Troy Hinckley
#+HUGO_BASE_DIR: .

* Main
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: .
  :END:

** DONE [#B] About
:PROPERTIES:
:EXPORT_FILE_NAME: about
:END:

I am an engineer with interests in Emacs, Programming languages, performance, and compilers. Currently working in hardware design at Intel in Colorado.

* Posts
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: post
  :END:

** DONE Using org mode to write email for outlook :org_mode:emacs:
:PROPERTIES:
:EXPORT_DATE: 2019-02-08
:EXPORT_FILE_NAME: outlook-email-in-org-mode
:END:
I see many threads on Reddit and blog posts about using email inside Emacs. I mean, I already have =org-mode= which organizing my whole digital life. But then all my work email is provided through outlook, which does not allow me to fetch email with anything other then their proprietary software.

Microsoft outlooked was designed to be used by people writing marketing emails, not people talking about code. There is no way to distinguish what is code from what is text, or call our programming symbols from the rest of the prose. Emacs =org-mode= on the other hand, was built for working in a technical environment.

Thankfully org makes it easy to export any heading to HTML. The hard part is getting that HTML into outlook. Most of the ideas presented here were taken this [[http://bnbeckwith.com/blog/org-mode-to-outlook.html][post]], and then expanded on.

The process I use to write my email with =org-mode= is as follows
1. write the email in an org capture buffer
2. Use a custom function to copy the exported HTML to the clipboard
3. go to outlook and use a custom VBA function to insert the HTML from
   the clipboard as formatted text

*** Setting up Emacs
**** Using an org capture buffer
Using an org capture buffer is perfect for writing email, because I can save it as a draft if needed, or export the contents and then throw the buffer away. Also, most of the time, the content of interest is what I am working on that moment, so I have everything at hand. Here is the simple template that I use to write emails.
#+BEGIN_SRC emacs-lisp -n
  (add-to-list
   'org-capture-templates
   '("e" "Email"
     entry (expand-file-name "email.org" org-directory)
     "* %?" :empty-lines 1))
#+END_SRC

**** Exporting from org-mode
Normally if you wanted to export an org header as HTML, you would use =C-c C-e= to open the export menu. =hH= will open a dedicated buffer with the HTML contents of your org file. From there you can copy the whole buffer. However I find it much faster to use this helper function (bound to =C-x M-e=).

#+BEGIN_SRC emacs-lisp +n
  (defun export-org-email ()
    "Export the current email org buffer and copy it to the
  clipboard"
    (interactive)
    (let ((org-export-show-temporary-export-buffer nil)
          (org-html-head (org-email-html-head)))
      (org-html-export-as-html)
      (with-current-buffer "*Org HTML Export*"
        (kill-new (buffer-string)))
      (message "HTML copied to clipboard")))
#+END_SRC

**** Better CSS for export
The default HTML exported by org is spartan to say the least. Thankfully it is pretty easy to define some custom to CSS that looks prettier and plays nicer with outlooks HTML rendering engine. The outlook compatible HTML I use is located [[https://github.com/CeleritasCelery/org-html-themes/blob/master/styles/email/css/email.css][here]]. The function below adds my CSS to =org-html-head=. It is called by =export-org-email= from the previous section.

As you can see in the function below, I have this CSS at the local path =~/org/org-html-themes/styles/email/css/email.css=. You will need to change this to where ever you keep the CSS file.
#+BEGIN_SRC emacs-lisp +n
  (defun org-email-html-head ()
    "Create the header with CSS for use with email"
    (concat
     "<style type=\"text/css\">\n"
     "<!--/*--><![CDATA[/*><!--*/\n"
     (with-temp-buffer
       (insert-file-contents
        "~/org/org-html-themes/styles/email/css/email.css")
       (buffer-string))
     "/*]]>*/-->\n"
     "</style>\n"))
#+END_SRC

*** setting up outlook
**** Getting the HTML into outlook
This is the tricky part. outlook does not make it easy to insert HTML inline. I had to learn some VBA and use the outlook code editor. I hope I never have to do that again.

To add a function to outlook
1. Press =Alt-F11= to bring up the VBA editor.
2. You should see the default project. Change this project name to something more appropriate. Note that the Project name *MUST NOT* be the name of the function (~PrependClipboardHTML~) so name it something else.
3. Right click on the project to add a new module and copy in the function from below

#+BEGIN_SRC vb
Sub PrependClipboardHTML()
    Dim email As Outlook.MailItem
    Dim cBoard As DataObject

    Set email = Application.ActiveInspector.CurrentItem
    Set cBoard = New DataObject

    cBoard.GetFromClipboard
    email.HTMLBody = cBoard.GetText + email.HTMLBody

    Set cBoard = Nothing
    Set email = Nothing
End Sub
#+END_SRC

**** Fix object library
This step may not apply to everyone, but in order to get this to work, I also had to add the =Microsoft Forms 2.0 Object Library= to the References. I figured this out by looking at [[https://www.reddit.com/r/orgmode/comments/74k2mx/org_link_to_message_within_outlook_2016/][this]] Reddit thread.

1. Click on =Tools= in the menu bar (or use =Alt-t=).
2. Select =References...=
3. Select =Browse...=
4. Browse to =C:\Windows\System32\FM20.DLL= and select open

**** Add to Quick Access Toolbar
This function only makes sense in context of an email. To enable it there, add it to the quick access toolbar at the top.

1. Press =Ctrl-n= to open up a new email.
2. Select the little down arrow at the very top for the =Customize Quick Access Toolbar= menu.
3. Select =More Commands=.
4. In the drop down for =choose commands from:= select =Macros=. You should see the =PrependClipboardHTML= macro you created here.
5. Add it to the right hand side pane with the =Add >>= button.
6. Click on =Modify...= to change the icon and display name. You can also use the arrow to change the ordering in the =Quick Access Toolbar=

Now clicking on that button will copy clipboard contents into the email as HTML. Our raw HTML exported from Org mode gets inserted nicely and we gain the formatting desired.

The other bonus (or maybe the main point) is that now you can also use a built-in shorcut for the Quick Access Toolbar commands to run this one. By pressing =Alt=, you can see a number by your command.

**** Matching the default font in Outlook
It is nice sometimes to have the default font in outlook match what you are exporting from org mode. To make this happen, do the following steps in Outlook.

1. On the =File= tab, choose =Options= > =Mail=.
2. Under =Compose messages=, choose =Stationery and Fonts=.
3. On the =Personal Stationery= tab, under =New mail messages= or =Replying or forwarding messages=, choose Font.
4. In the =Font= box, choose the font, style, size, and color that you want to use. You can see a preview of your changes as you make them.
5. Choose =OK= three times to return to Outlook.

*** Bonus Content
Here are a few more ideas that are not necessary for this workflow but
are useful to me.

**** More advanced VBA
The =PrependClipboardHTML= function I showed above is not actually the version I use. But I chose to mention present it as the solution because it is simple and works well. This more advanced version has two differences

1. Works with inline email replies
2. If the subject line is empty, the HTML header at the start of the body is used as the subject line. This allows you add the subject line in org-mode and have it automatcially inserted.

#+BEGIN_SRC vb
  Sub PrependClipboardHTML()
      Dim email As Outlook.MailItem
      Dim cBoard As DataObject

      Set email = GetCurrentItem()
      Set cBoard = New DataObject

      cBoard.GetFromClipboard

      Dim sText As String
      Dim headerStart As Integer
      Dim headerStartClose As Integer
      Dim HTMLPre As String
      Dim HTMLPost As String
      Dim subject As String
      Dim paragraphStart As Integer

      Dim headerEndStr As String
      Const titleText = "<h1 class=""title"">"

      headerEndStr = "</h1>"
      headerStart = Len(titleText)

      sText = cBoard.GetText
      HTMLPre = sText

      headerStart = InStr(sText, titleText)
      If Not headerStart > 0 Then
          ' if no title text, we use the starting header
          headerStart = InStr(sText, "<h2 id=")
          headerEndStr = "</h2>"
      End If

      ' we use the first header as the subject line if the subject line is empty
      If headerStart > 0 Then
          headerStartClose = InStr(headerStart, sText, ">") + 1
          Dim headerEnd As Integer
          headerEnd = InStr(headerStartClose, sText, headerEndStr)
          If headerEnd > 0 Then
              subject = Mid(sText, _
                  headerStartClose, _
                  headerEnd - headerStartClose)
              HTMLPre = Mid(sText, 1, headerStart - 1) ' arrays start at 1...
              HTMLPost = Mid(sText, headerEnd + Len(headerEndStr))
          End If
      End If

      email.HTMLBody = HTMLPre + HTMLPost + email.HTMLBody
      ' only use the HTML subject if an email subject is absent
      If Len(email.subject) = 0 Then
          email.subject = subject
      End If

      ' deallocate objects
      Set cBoard = Nothing
      Set email = Nothing

  End Sub

  ' Get either the active email item or the current window
  Function GetCurrentItem() As Object
      Dim objApp As Outlook.Application

      Set objApp = Application

      On Error Resume Next
      Select Case TypeName(objApp.ActiveWindow)
          Case "Explorer"
              Set GetCurrentItem = objApp.ActiveExplorer.ActiveInlineResponse
          Case "Inspector"
              Set GetCurrentItem = objApp.activeInspector.CurrentItem
      End Select

      Set objApp = Nothing
  End Function
#+END_SRC
**** Normalize outlook formatting
Unless you disable it, outlook will try and "prettify" some characters as you type with non ascii-compatible versions. This means that you will often encounter errors when copying code out of outlook and trying to paste into a shell or source file. The following function takes the last paste normalizes it to be ascii compatible.

#+BEGIN_SRC emacs-lisp
  (defun normalize-text (beg end)
    "normalize characters used in Microsoft formatting"
    (let* ((orig-text (buffer-substring beg end))
           (normalized-text
            (thread-last orig-text
              (replace-regexp-in-string "–" "--")
              (replace-regexp-in-string (rx (char "‘’")) "'")
              (replace-regexp-in-string (rx (char "“”")) "\""))))
      (unless (equal orig-text normalized-text)
        (save-excursion
          (goto-char beg)
          (delete-region beg end)
          (insert normalized-text)))))

  (defun normalize-region (beg end)
    "normalize the last paste, or if region is selected, normalize
  that region."
    (interactive "r")
    (if (region-active-p)
        (progn (normalize-text beg end)
               (deactivate-mark))
      (apply #'normalize-text
             (cl-sort (list (point) (mark t)) '<))))
#+END_SRC
**** Have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/gxrg0b/writing_email_for_outlook_inside_emacs/?utm_source=share&utm_medium=web2x&context=3][Reddit]] or send me an [[mailto:troy.hinckley@dabrev.com][email]]

** DONE Native shell completion in Emacs :shell:emacs:
:PROPERTIES:
:EXPORT_DATE: 2020-01-04
:EXPORT_FILE_NAME: native-shell-completion-in-emacs
:END:
I am obsessed with autocompletion in shell mode. Running a shell in ~shell-mode~ instead of a terminal emulator has so many advantages. You can treat the whole buffer just like a normal Emacs buffer. You can copy and paste and edit the line normally. You can hook it into native Emacs functionality. You can even [[https://github.com/riscy/shx-for-emacs][display images!]]

However there is one big disadvantage. You lose access to the state the shell. This means that you have to do tricks like ~shell-dirtrack-mode~ just to make sure you are in the right directory. It also means that all the native shell completions are not available. I have tried to approach this problem from multiple angles with packages like [[https://github.com/CeleritasCelery/company-fish][this]], [[https://github.com/CeleritasCelery/company-async-files][this]], and [[https://github.com/CeleritasCelery/company-arguments][this]]. (Yes, I have written a half dozen modes to try solve this.) The most popular package to try and solve this is [[https://github.com/szermatt/emacs-bash-completion][bash-shell-completion]]. However all these solutions have the same problem that they don't actually reflect the internal state of the shell, they are just close approximations.

But the shell knows its own state. In a normal terminal emulator, tab completion will give you access to it. When looking at my shell buffer, it seems that the information I want is just below the surface. If only there was some way to access it. I am not the only one who has had this thought. There is a [[https://stackoverflow.com/questions/163591/bash-autocompletion-in-emacs-shell-mode][stackoverflow]] with over 26,000 views asking this same question. But no one has managed to access the native tab completion before. So I determined to solve this once and for all.

*** The curious case of bash
If you use csh and send some text followed by the tab character, it will print out all possible completions. But not bash. Try this code and you get nothing.
#+BEGIN_SRC lisp
(comint-simple-send (get-buffer-process (current-buffer)) "git\t\x15")
#+END_SRC

It works in the terminal but not in the Emacs shell. What conspiracy is this? Turns out that Emacs and bash have put a lot of effort into making sure completion does not work. The first thing to notice is that ~explicit-bash-args~ contains the argument ~--no-editing~, which will disable readline completion. Let get rid of that shall we?
#+BEGIN_SRC lisp
  (setq explicit-bash-args
            (delete "--noediting" explicit-bash-args))
#+END_SRC

However removing that still does not enable tab completion. There must be something else going on here. This time it is on the bash side. Looking in the source code we see the follow block.
#+BEGIN_SRC c :hl_lines 21
  term = get_string_value ("TERM");
  emacs = get_string_value ("EMACS");
  inside_emacs = get_string_value ("INSIDE_EMACS");

  if (inside_emacs)
    {
      emacs_term = strstr (inside_emacs, ",term:") != 0;
      in_emacs = 1;
    }
   else if (emacs)
     {
       /* Infer whether we are in an older Emacs. */
       emacs_term = strstr (emacs, " (term:") != 0;
       in_emacs = emacs_term || STREQ (emacs, "t");
     }
   else
     in_emacs = emacs_term = 0;

  /* Not sure any emacs terminal emulator sets TERM=emacs any more */
  no_line_editing |= STREQ (term, "emacs");
  no_line_editing |= in_emacs && STREQ (term, "dumb");
#+END_SRC

For some reason that I can't explain, bash has special code for running inside Emacs. Lo and behold, if ~$TERM~ is ~dumb~ and ~$INSIDE_EMACS~ is set, then line editing is disabled by the shell itself. Any reason for this? I would love to know. Changing ~$TERM~ to something other then ~dumb~ fixes the issue, but then programs might not interpret our terminals capabilities correct. The best thing to do is remove the environment variable ~$INSIDE_EMACS~. Doesn't seem to do anything useful after all.
#+BEGIN_SRC lisp
(advice-add 'comint-term-environment
            :filter-return (lambda (env) (cons "INSIDE_EMACS" env)))
#+END_SRC

And with that we have working tab completion in bash!

*** Making completion "Emacsy"
We could stop here and just create a function to send tab to the underlying process. It would behave exactly like the terminal does. But this is Emacs. The whole reason we are using ~shell-mode~ in the first place is because things in Emacs are nicer then things in the terminal. We have things like ~completion-at-point~ and ~company-mode~ that would make a terminal emulators head spin. Makes sense to take advantage of that. So I created a new package called [[https://github.com/CeleritasCelery/emacs-native-shell-complete][native-complete]] that talks to the underlying process and gets the actual completions that the shell would natively provide. No more trying to use other packages to /guess/ the shells state, or starting another process that may be out of sync. It even supports invoking subshells! This effort is still work in progress, and many shells have yet to be tested. As with many things, it is not as simple in the implementation as it should be.

I hope this is the shell-mode completion Endgame. I don't think I can take much more.

**** Have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/ek8v0e/native_shell_completion_in_emacs/?utm_source=share&utm_medium=web2x&context=3][Reddit]] or send me an [[mailto:troy.hinckley@dabrev.com][email]]
** DONE [#B] When pure function lie :design:emacs:
:PROPERTIES:
:EXPORT_DATE: 2021-04-07
:EXPORT_FILE_NAME: when-pure-functions-lie
:END:
Here is a simple question. Given the lisp function below (and that the function is not advised) what will the output be of ~(foo)~?
#+begin_src emacs-lisp
  (defun foo ()
    "foo")
#+end_src

Seems pretty simple right? How could the answer be anything other then ~"foo"~? It is just returning a constant string. Or is it? The real answer to this question is...

We have no idea. It can could be any string of length 3.

How can this be? Well it turns out that constant literals in lisp (common lisp, emacs lisp, etc) are not constant. They can be modified at runtime like any other variable. Here is an elisp example that does just that.
#+begin_src emacs-lisp
  (defun foo ()
    "foo")

  (foo) => "foo"

  (defun bar ()
    (let ((x (foo)))
      (aset x 0 ?福)
      x))

  (bar) => "福oo"
  (foo) => "福oo"
#+end_src

Look at that! We have modified the behavior of another function using only its return value. We might have considered ~foo~ a [[https://en.wikipedia.org/wiki/Pure_function][pure function]]. However this cannot be the case because we can return different values for the same input.

This is true whether it's dynamic or lexically bound, interpreted or byte compiled. And there is no way to get the original literal value back without redefining the function. Which means that once you try and instrument the function to see what is going on, the problem magically goes away! Here is another example where we are able to change a different constant literal in the same function. Since ~foo~ is byte-compiled, the two constants are mapped to the same mutable object. So changing one changes the other!
  #+begin_src emacs-lisp
    (defvar global)

    (byte-compile
     (defun foo (x)
       (setq global "foo")
       (concat "foo" x)))

    (foo "bar") => "foobar"

    (aset global 2 ?x)

    (foo "bar") => "foxbar"
#+end_src

*** Why does this happen?
Strings in lisp are mutable, and when you pass a string to something it actually passes a reference to it. But once you have a reference to a string, you can modify it however you want. Which means that you are modifying the original string that was part of the constant list of the function!

As far as I could find, this behavior is unique to lisp (and it works with literal lists and vectors as well). Other dynamic languages don't have this property. For example, the python code below does not change the original list[fn:1].
#+begin_src python
  def foo():
      return [1, 2, 3]

  def bar():
      x = foo()
      x[0] = 0
      return x

  print(foo()) => [1, 2, 3]
  print(bar()) => [0, 2, 3]
  print(foo()) => [1, 2, 3]
#+end_src

But the elisp version will
#+begin_src emacs-lisp
  (defun foo ()
    '[1 2 3])

  (defun bar ()
    (let ((x (foo)))
      (aset x 0 0)
      x))

  (foo) => [1 2 3]
  (bar) => [0 2 3]
  (foo) => [0 2 3]
#+end_src

I would venture to say that anytime you overwrite a constant literal it almost certainly a bug. And a very hard to debug one at that! So why does lisp allow this? I imagine a large part of it is simplifying the implementation. You don't have to check if the value you are modifying is constant or not, you just mutate it. Another part of it is that Common Lisp was conceived in a time when string where just vectors of ascii characters (Similar to C). That made modifying them like modifying normal arrays. But now with the advent of unicode, changing a "character" of string is not so easy. It can involve shifting the entire string or even reallocating depending on the size of the code point. This is why most modern languages have immutable strings by default.

This is another reason that it is hard to make a true multi-threaded elisp. You can't share function between threads when "normal" code can change the behavior of functions being used in different threads. Functions that look pure might be changing under the hood. In this situation you are cross-pollinating your code with mutable data. I should add that this really has nothing to do with [[https://en.wikipedia.org/wiki/Homoiconicity][homoiconicity]]. You could still have a fully homoiconic language without the ability to overwrite constant literals.

When I first saw this behavior, I thought for sure that I found a bug in the language implementation. This couldn't possibly be intentional. But after investigating more, I found that this was expected... at least if you are lisp programmer.
****  Have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/mm70re/when_pure_functions_lie/?utm_source=share&utm_medium=web2x&context=3][Reddit]] or send me an [[mailto:troy.hinckley@dabrev.com][email]]

[fn:1] As several people [[https://www.reddit.com/r/emacs/comments/mm70re/when_pure_functions_lie/gtq1oir?utm_source=share&utm_medium=web2x&context=3][pointed out]], the reason this works in python is because it is making a copy of the list every time it returns. You could introduce the same issue in python using default arguments, which are not copied. To protect against this in lisp, you can use ~(vector 1 2 3)~ instead of ~'(1 2 3)~ and it will make a copy of the vector.

** DONE [#B] Building an Emacs lisp VM in Rust :emacs:rust:
:PROPERTIES:
:EXPORT_DATE: 2021-10-21
:EXPORT_FILE_NAME: building-an-emacs-lisp-vm-in-rust
:END:

Updated: 2023-01-06

About a year ago I was bitten by the PL bug. It started with reading [[http://craftinginterpreters.com/][Crafting Interpreters]] and discovering the wonders hidden under the hood of a compiler. I am also been a big fan of Emacs, and this started to get me interested in how its interpreter works. At the same time, I was reading the Rust book and trying to understand the concepts there. This all came to a head, and I decided to write an Emacs Lisp interpreter called [[https://github.com/CeleritasCelery/rune][rune]] in Rust.

My goal for this project is to bootstrap [[https://github.com/emacs-mirror/emacs/blob/master/lisp/emacs-lisp/bytecomp.el][bytecomp.el]] and use the Emacs compiler with my bytecode vm using only stable Rust. I have not reached that goal yet, but I have bootstrapped several core Emacs lisp files. At this point I have a enough of an interpreter that I want to share an update and mention some of the trade-offs and other things I have learned.
*** Overview
**** Tree walk or bytecode?
Emacs has 3 separate execution engines: a tree walk interpreter, a Bytecode VM, and ([[https://git.savannah.gnu.org/gitweb/?p=emacs.git;a=commit;h=289000eee729689b0cf362a21baa40ac7f9506f6][recently!]]) native compile. They all provide their own sets of trade-offs, but that also means that any new feature needs to be implemented up to 3 times. I didn't want the duplicate work, so I opted to only have a byte code VM and no interpreter. This turned out to be harder than I initially thought. All the early elisp files assume that you are using an interpreter. Macros are often used before they are defined because the interpreter has lazy-macro expansion. This is harder for a byte-compiler because you want to expand the macro's at compile time instead of run time. I ended up needing to make some tweaks to the ordering and structure of the [[https://github.com/CeleritasCelery/rune/tree/master/lisp][lisp files]] to support a bytecode-only bootstrap.
**** Object representation
A critical part of any dynamic language is how to represent types at runtime. Since you will frequently be boxing and unboxing values, you want these to be both time and space efficient.

Rust provides a strong candidate in its enums, but you are limited to the representations that they provide. Most of the time this isn't a problem. However, because of the language specification that pointers are a full word, you can’t normally use optimizations like [[https://piotrduperas.com/posts/nan-boxing][NaN-boxing]] or [[https://en.wikipedia.org/wiki/Tagged_pointer][pointer tagging]] in Rust. Therefore, I was initially defining an object type as a ~union~.
#+begin_src rust
  union Object<'ob> {
      tag: Tag,
      data: i64,
      marker: PhantomData<&'ob ()>,
  }
#+end_src

Then in the boxing and unboxing code, I could check the type of the tag field and reinterpret the bits as whatever type was needed. This had the advantage of being extremely flexible (since I had complete control over the bit layout and representation) but it also had some drawbacks compared to a proper Rust enum.

1. Unboxing requires ~unsafe~ code
2. Must manually match the tag to the right data type. There are no compiler checks here.
3. No way to ~match~ directly on the union. Need to create an accessor functions to get the underlying value as an enum.
4. Can't use variants as values. With an Enum you can use ~Option::None~ or ~Option::Some(T)~ as values, but instead you have to create constants to represent common values.
5. No debugger support. A union is completely opaque to the Rust debugger.
6. No destructuring support. Since Rust's pattern syntax does not understand my type, there is no way to do the following
#+begin_src rust
  match object {
      Some(Object::Nil | Object::Int(_)) => ...,
      Some(Object::String(s)) => ...,
      _ => ...
  }
#+end_src
***** A better solution
The [[https://crates.io/crates/enum-ptr][enum-ptr]] crate provides a good way to address this. We have a tagged version of our value that will "untag" into a regular Rust enum. The "tagged" value is just a pointer and a marker.
#+begin_src rust
  struct Gc<T> {
      ptr: *const u8,
      _data: PhantomData<T>,
  }
#+end_src

We can encode our tag in the pointer however we wish. For my project I am shifting the value by one byte and storing the tag in the low bits. You can then define a ~tag~ and ~untag~ function to convert between ~Gc<Object>~ and ~Object~[fn:5].

This makes the type ergonomic to use, because we can use it like a normal Rust enum. But we still get the advantages of the type being more compact. The only real downside here is that we can't implement ~Deref~, because the type signature requires that you return a reference, and we need to return an owned value. If the ~Deref~ trait used GAT's instead of references, we wouldn't have this limitation.

**** Defining functions
I can’t take credit for this, as the idea came from [[https://github.com/remacs/remacs/tree/master/rust_src/remacs-macros][remacs]] (the original Emacs in Rust project), but it really showcases the power of Rust procedural macros. The ~defun~ macro is applied to any normal Rust function and it then becomes callable from lisp.

#+begin_src rust
  #[defun(name = "-")]
  pub(crate) fn sub(number: Option<Number>, numbers: &[Number]) -> NumberValue {
      match number {
          Some(num) => {
              let num = num.val();
              if numbers.is_empty() {
                  -num
              } else {
                  numbers.iter().fold(num, |acc, x| acc - x.val())
              }
          }
          None => Int(0),
      }
  }
#+end_src

This macro creates a wrapper around the function that transforms lisp objects into Rust types and back, handling any type errors along the way. The type signature of the Rust function also gets converted to the type signature in lisp; ~Option~ types become ~&optional~ and slices become ~&rest~. For example the function signature above will become ~(- &optional NUMBER &rest NUMBERS)~. This makes it easy to use the function in Rust or lisp, and the syntax is much cleaner then the [[https://github.com/emacs-mirror/emacs/blob/3cabf64131b93e1b0510a01bcce8efde38b20bc4/src/lisp.h#L3050][DEFUN]] macro used in the the Emacs C code.

*** Interesting learnings along the way
**** Generics in Rust
Generics are a really powerful feature that let you build reusable data structures and help eliminate some boilerplate. Less code means less bugs. Generics are particularly useful in conjunction with traits, letting you implement them for a range of types. However, I found that in practice generics were less useful than they could have been due to the lack of [[https://rust-lang.github.io/rfcs/1210-impl-specialization.html][specialization]]. This absence means that anytime you need to specialize for one type you completely lose the use of generics for that function/trait [fn:7]. Because of this I ended up implementing many of the traits with macros instead of generics. If specialization is ever stabilized, it will remove hundreds of lines of boilerplate from the code base. But it looks like that is still a [[https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/][ways off]].

**** Garbage collection
I have not currently implemented a garbage collector for my interpreter, though it doesn't leak memory. This works because all objects are “owned” by a ~Context~, and all the lifetimes of objects are tied to the borrow of that context. So when the context goes out of scope, so do all the objects it owns. This works fine for bootstrapping, but there is no freeing of unused memory. I have done a lot of reading on garbage collectors and they are considered very tricky to get right. As Bob Nystrom [[http://craftinginterpreters.com/garbage-collection.html#garbage-collection-bugs][said]], “garbage collector bugs really are some of the grossest invertebrates out there.”

Rust has some unique offerings that promise to not only make garbage collectors easier to implement, but safer to use as well. I am not going to go into detail here because you can find a great overview of different approaches in this [[https://manishearth.github.io/blog/2021/04/05/a-tour-of-safe-tracing-gc-designs-in-rust/][series of blog posts]].

The most interesting crates to me are ones that use Rust's borrow checker to ensure that it is safe to run the collector. All objects have a lifetime tied to a ~Context~ object. Anytime the Garbage collector runs it takes a ~&mut self~, ensuring all objects it created can't be accessed afterwards. In order to keep objects alive you need to root them. This is done with either a stack or linked list. Some examples of this approach are [[https://github.com/asajeffrey/josephine][joesphine]] and [[https://github.com/withoutboats/shifgrethor][shifgrethor]].

Another similar approach is the concept of [[https://raw.githubusercontent.com/Gankro/thesis/master/thesis.pdf][generativity]], which is essentially using a unique lifetime to brand objects so they cannot be unified with other Context's. The [[https://crates.io/crates/gc-arena][gc-arena]] and [[https://crates.io/crates/cell-gc][cell-gc ]]are example of this. One thing is for sure, these libraries will become much easier to use if Rust ever gets the ability to track stack roots[fn:1]. Until that time there is still a wide space to be explored.

The last thing that makes garbage collectors difficult in Rust is the that the [[https://github.com/rust-lang/wg-allocators][allocator API]] is still unstable, and probably won't be stabilized anytime soon. Some gc algorithm's rely on particular layouts of data to work correctly. Currently you need to either use the changing API in nightly or implement things yourself with pointers.

**** Fixing lifetime issues
When I first created lisp objects, they were unions with raw boxed pointers. After all, this is what you would have in C. However, after facing several memory errors, I decided to take advantage of Rust lifetime system and add lifetimes to all objects. They now hold a ~PhantomData~ of a reference. When I first made this change it lead to a lot of pain. I learned it is very valuable to really step back and actually think about borrow checker messages. Oftentimes rather then fighting the borrow checker, you are better off restructuring your code to make it more lifetime friendly. Once I did a major refactor where data flows from main to the rest of the program most of my lifetimes issues just disappeared. Another thing to keep in mind is that just because Rust /allows/ your lifetimes doesn't make them /correct/. All that rustc cares about is that your lifetimes are not memory unsafe; it doesn’t care if they are [[https://github.com/pretzelhammer/rust-blog/blob/master/posts/common-rust-lifetime-misconceptions.md/#5-if-it-compiles-then-my-lifetime-annotations-are-correct][correct]]. It is up to you as the developer to make sure your lifetimes are right. Most often what I needed to do to correct my lifetimes was to split them up. Forcing Rust to unify unrelated lifetimes is guaranteed to cause more pain then needed.

**** Globals vs multi-threaded
I was initially inspired to do this project by /Crafting Interpreters/ and reading the Emacs internals. Both of these programs make heavy use of globals to store and manipulate state, which is very common in C. However Rust takes a different stance. In Rust there is no such thing as single threaded code. Even code that does not rely on any concurrency constructs is expected to work without issues in a multi-threaded environment. This means all globals must be wrapped in a concurrency safe type.

However, I was still convinced that I wanted to do things the "C" way. It made following my templates (Lox and Emacs) much easier. Accessing a raw global is cheap; Accessing it though a mutex is not. I "knew" that my interpreter was not multi-threaded and I did not want to pay that overhead. However, finding out how to implement raw globals was no easy task. It took some digging, but I did discover that you can implement C-style zero-cost globals in Rust with some unsafe code. Not too long after I implemented that I began to run into random test failures. I found much to my surprise that even the test runner in Rust is multi-threaded! At this point, I broke down and decided to get on board with the Rust approach to concurrency. I moved all globals to the stack or put them behind a mutex. It wasn’t as bad as I feared.

***** The seeds of parallelism
As part of the move to a concurrency safe runtime, I started thinking about what it would take to have a true multi-threaded Emacs lisp. To experiment with this, I set it up so that all functions are shared between threads with atomics; But values are thread local. This brought up some interesting challenges that Emacs lisp presents to concurrency, all related to mutability and aliasing.

For one, [[https://coredumped.dev/2021/04/07/when-pure-function-lie/][function literals are mutable in lisp]]. This means you can change a function by mutating it’s return value. If functions are shared between threads, then they can't be mutable; Otherwise you expose yourself to dataraces. In Common Lisp they just say "yolo!" and make mutating a function literal undefined behavior. However, you can’t easily tell when you are doing this; It can often be very far from the call site.

Another issue is that aliasing is very common in elisp. This generally isn’t an issue in single-threaded code, but becomes a source of very difficult bugs in a multi-threaded world. You need to either make all objects concurrency safe (which is very expensive) or prevent threads from mutably aliasing each other’s data. This is one of the areas where Rust really shines, but would require a lot of hard trade-offs in lisp.

For example, concurrency in Emacs would not be very useful without the ability to share buffers. If you share buffers, you also need a way to share buffer local variables; and buffer local variables can share data (cons cells, strings, and vectors) with other local variables. There is no way to share a buffer with another thread without also sharing your entire environment. At some point I plan to write more about potential multi-threading in Emacs, but that will have to be saved for a future post.

*** Rust as a language backend
Overall, I have come to love Rust! It makes systems programming feel accessible. And the community is absolutely awesome [fn:2]. I've never had a question that I was not able to get help with. That being said, implementing an interpreter for a dynamic language in Rust is particularly challenging because the host language does not[fn:3] follow Rust's rules around mutability and aliasing. To solve this you need to either do runtime accounting using ~Rc<RefCell<T>>~ (which is expensive and leaks cycles), or deal with upholding all of Rust invariants manually in unsafe code. Neither is a very attractive proposition.

Speaking of unsafe, you often hear that writing unsafe code is "just like writing C". That is not really true. Rust has more invariants that need to be upheld then does C, especially related to mutability, aliasing, traits, layout, initialization, and dropping. All these invariants need to be considered when writing unsafe code and can lead to [[https://www.youtube.com/playlist?list=PLqbS7AVVErFj1t4kWrS5vfvmHpJ0bIPio][very tricky unsound behavior]]. Many of these are either not a concern, or much less of a concern, in correct C code.

Rust also lacks a feature of C that is used to implement fast interpreter loops; [[https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables][computed goto]]. This feature can be used to implement [[https://en.wikipedia.org/wiki/Threaded_code#Direct_threading][direct threading]] without the need for assembly code, giving a sizable [[http://www.cs.toronto.edu/~matz/dissertation/matzDissertation-latex2html/node6.html][performance]] increase on some processors[fn:4]. Rust may support this in the future, but given the complex interactions this would have with the borrow checker, I doubt it. I could see future where fast Rust interpreters write their inner dispatch loop in C just to take advantage of this feature.

Now, none of this is to say that Rust is poor language for writing a dynamic language backend. On the contrary, it offers some features like sum types, unnullable pointers, and safety from concurrent data races that are really powerful. However, some of Rust's core strengths in aliasing and mutability apply less well to this domain then they do to others.

*** Conclusion
I really gained an appreciation for the depth of the Emacs internals. That code has been around for a long time and is very mature; but at the same time, it is also under active development. Trying to implement Emacs from scratch would mean not only matching the current well-tested functionality, but also trying to keep up with the constantly changing internals. While Emacs may not be the most elegant C code base, it is certainly robust.

As for how long I plan to continue this project, I don't really know. At very least I am going to bootstrap the Emacs lisp compiler to test it against my runtime and implement a garbage collector. My expectation is either that I will learn enough about text-editors and interpreters to be able to contribute to Emacs proper, or I will find a problem in the Rust ecosystem that does not have a good solution and focus on that instead. Or I may continue to see how far I can push this project. Either way, contributions and testing are welcome. Please take a look at the [[https://github.com/CeleritasCelery/rune][code]] and give feedback. I am particularly interested in anything that could be unsound or lead to undefined behavior. This has been a great experience and I am learning more than I could have hoped.

**** Have a comment?
Join the [[https://discu.eu/?q=https%3A%2F%2Fcoredumped.dev%2F2021%2F10%2F21%2Fbuilding-an-emacs-lisp-vm-in-rust%2F][discussion]], or send me an [[mailto:troy.hinckley@dabrev.com][email]]

[fn:1] LLVM has support for this, but is has not been moved into Rust yet.

[fn:2] That is, so long as you don't use a trigger phrase like "unsafe code" or "this works fine in C".

[fn:3] Some functional languages do have invariants around immutability, but they often use mutability under the hood.

[fn:4] I saw some claims that using threaded dispatch in CPython brought a 10-20% improvement, but I didn't see benchmarks.

[fn:5] As an added bonus converting between objects can be a no-op with the [[https://github.com/rust-lang/rust/issues/60553][arbitrary_enum_discriminant]] feature that was [[https://blog.rust-lang.org/2022/12/15/Rust-1.66.0.html#explicit-discriminants-on-enums-with-fields][released with Rust 1.66]].

[fn:7] This [[https://github.com/rust-lang/rust/issues/50133][issue]] shows how a seemingly innocent blanket implementation in the core can break a bunch of generics for all users due to no specialization.

** TODO [#B] compilation mode on steroids
=compile.el= is a built-in package for Emacs that Let's you run
compilations in special dedicated mode. For example, if we wanted to
run make on a project, we would call ~M-x compile~ and it would
display a command prompt which conveniently defaults to "make -k". We
could use to run other commands as well, such as ~g++~, ~go~, or
~javac~. Basically anything that you can run from the command line,
you can run in compile mode.

Once you launch the command, compile opens a new popup window that
shows the commands progress. It actively scans for errors using a
builtin list and reports the total error count in the mode line. At
any time you can navigate to an error and use ="RET"= to jump to the
file. It works great with almost no configuration for the majority of
cases.

My builds, however, are more complicated. I use a proprietary set of
build tools that might run for over 10 hours. None of the standard
error parsing regex applied to me. Since my builds take so long, I
would often have up to a dozen builds running at once. There are also
several different types of builds that need to run particular order,
assuming the previous stage passed. On top of that, some of the builds
would stall and wait for user input forever. The compilation system as
it currently stands required too much micro management and was not
powerful enough to support my workflow. So for a long time, I managed
all my builds from command line, But this was also a very slow
workflow. So I decided to investigate what compile.el could do for me.
Eventually I was able to create a workflow with smart command
dispatching, command queuing, and a compilation status popup.

Before I get started let me just say that since all my build commands
are proprietary, There is little value in showing the actual arguments
and commands used. I think that would just create additional noise
anyway, so I am replacing all of that with sudo code. It should make
it easier to adapt this workflow to your needs.

*** dispatching commands
     My first issue was that I was frequently changing the commands
     line arguments. This would require me to

*** error parsing

*** detecting stalls

*** the command queue

*** alerts when a command finishes

*** the status window

*** log file mode

** TODO [#B] Emacs in 10 years
Emacs is the editor of a lifetime. It has been around more then 40 years, and I hope it will be around another 40. There are so many editors that come in with new tech and fancy features, but within a decade or two. But I hopefully will be able to use Emacs for the rest of my life. The editor is always adapting and changing. Here I put down some of my thoughts on what I hope the future of Emacs holds, and also what I hope it does not.
*** GUI first, Terminal second
Like many old applications, Emacs started in the terminal and that is where it takes it roots. This was the only option before the advent of GUI's. And even after graphical interfaces became popular there were still so many advantages to running in a simple terminal environment. But times have changed and there are now many advantges to using Emacs as GUI.  There is a popular [[https://blog.aaronbieber.com/2016/12/29/don-t-use-terminal-emacs.html][article]] by Aaron Bieber that explains some of the reasons, so I won't reiterate those here. But I hope the future of Emacs is focused on GUI first, Terimal second. You can already see this happening with things like Harbuff and better ~pixel-scroll-mode~ support being added to Emacs 27.
**** scrolling and cursor stays put
Scrolling in Emacs has always felt awkward compared to what you would get in a normal gui application. For one thing, you cannot scroll a window without also moving the cursor. And there is no easy way to get the cursor back unless you remembered to set a mark first. This need not be.

**** draw a window anywhere
If you want to do a fully stylized popup you need to either use an overlay or a child frame. Overlays have the limitation that they cannot cross window boundaries and only work with monospaced fonts. Using frames means you popup is dependent on your window manager. For me this means that every time I show or hide a child frame it flashes white before it redraws, leading to a very jarring experience. Emacs should be able to draw a floating window anywhere in the frame that can be used to tool-tips, completion, or anything else.
**** give me back my keys!
For historical reasons, terminals treat many different keybindings as the same. For example in a terminal emulator =C-j= and =C-m= are  =<return>=, =C-i= is =<tab>=, =C-[= is ~<esc>~, etc. Emacs carries these same terminal mappings into the GUI, even though the GUI can distinguish these keys. Fixing this requires some [[https://emacs.stackexchange.com/questions/220/how-to-bind-c-i-as-different-from-tab/221#221][voodoo]] with ~input-decode-map~. The GUI version of emacs should distinguish these keys, but for compatibility they could be bound to the same function by default. But that makes it easy for users to rebind them if needed.
*** Better buffer model
**** Long lines
A common pain point with Emacs is handling files with long lines. There is a [[https://www.emacswiki.org/emacs/SoLong][new package]] in Emacs 27 that will check for long lines in a file and disable features if they are over a certain length. But this still does not get to the heart of the problem. There is a whole [[https://github.com/codygman/figure-out-emacs-long-lines-issue/blob/master/figuring-out-emacs-display-issues.org][github repo]] dedicated to trying to solve this issue. However it looks like it would need a complete rework of the Emacs buffer model.
**** Code folding
If you try and fold more then a few thousand lines Emacs will slow to a crawl. Vim and sublime can handle this fine, and many users of those editors will enable folding by default. But in Emacs that is a time bomb waiting to happen. This seems related to the work Emacs has to do every time you move the cursor. I have never been able to find a mode (including ~set-selective-display~) That is performant on large files. And large files it where you need folding the most.
*** modern runtime
**** jit compiler
There have been several attempts to create a JIT compiler for Emacs with the most [[https://lists.gnu.org/archive/html/emacs-devel/2018-08/msg00393.html][recent]] one being back in 2018. JIT stands for "Just in Time" compiler that will interprent new code straight byte-code. Currently byte code is only created if you explicityly byte-compile a function or file. This is a feature of many modern dynamic languages and could greatly improve the performance of evaluated elips code.
**** byte code optimization
Steve Yegge has a very interesting talk entitled [[http://steve-yegge.blogspot.com/2008/05/dynamic-languages-strike-back.html][Dynamic Languages Strike Back]] where he outlines some of the interesting advantages dynamic languages have over static ones. This advantage comes down to dynamic languages having a runtime process that optimize things on the fly, whereas a static language has to know everything at compile time. For example There is the concept of Trace Trees, where the runtime examines "hotspots" in the code and applies heavy optmiization to the most executed branches of that. This includes things like inlining function calls, tail call optimization, and caching values. There are a lot of really cool things that can happen with runtime environement like emacs.
Trace trees, tail call, string interning, inlining
fib(40) in python : 34
fib(40) in emacs : 131
fib(40) in c: 0.6 seconds
almost 4x worse performace
**** not stop-the-world GC
GC is a huge part of the total cost

53 seconds of GC.
13261 collections
40% of it is GC
**** make sure not to lose introspection

**** reader macros

http://xkcd.com/1638/

="\\`\\\\\\(\\(a\\|b\\|c\\)\\(d\\|e\\)\\\\)\\'"=
=r"\`\\((a|b|c)(d|e)\)\'"=

="\\(\\`\\|[^\\]\\)\\(\\\\\\\\\\)*\\(\\\\\\?\\)\\'"=
=r"(\`|[^\])(\\\\\)*(\\\?)\'"=

**** move c code to elisp
Example of line numbers

*** What I hope the future does not hold

**** concurency
Leads to sloppy code
there is a lot of headroom on in the single threaded space.
Maybe limit it to font locking and buffer refresh

**** first class extension languages
Vim and neovim can do this.

Those languages will fade. Emacs is the editor of a life time, it needs to stick around. And be comptaible long after those languages are gone.

no brow

****  browser in emacs
*** conclusion
Make sure that you praise it. Excited.

=C-c C-f=
** DONE Taking org-roam everywhere with logseq
:PROPERTIES:
:EXPORT_DATE: 2021-05-26
:EXPORT_FILE_NAME: taking-org-roam-everywhere-with-logseq
:END:
Updated: 2022-08-03

I love [[https://www.orgroam.com/][org-roam]]. It lets me take notes in a way that matches how I think. It makes it easy to recall what I have learned and find connections between ideas. But there has always been one big problem with org-roam: it ties me to the desktop. When I am on the go and all I have is my phone, I don't have access to my notes.

There are some stop gap solutions to try and fix this. [[https://beorgapp.com/][Beorg]] is iOS app that supports editing org files on mobile. However, it is more focused on task management than note taking. It does not offer text search, has no way to insert "org-roam-links", and does not supported nested directories of org files. [[https://organice.200ok.ch/][Organice]] is another org mobile solution that can be used as PWA. However, it suffers from the same limitations as beorg, and has a poor editing experience (when you switch away from the app it will close your text box, making it hard to take notes on something you are reading). In the end, I was not satisfied with anything that I found: nothing allowed the same workflow I had on desktop.

Then I happened upon logseq. It is open-source, privacy-centric note taking app inspired by org-mode and [[https://roamresearch.com/][roam-research]]. It has native support for the org mode format. Since both org-roam and logseq are based around roam itself, I can use the same workflow on both apps. It has all the features you would expect of a roam replica, including full text search, page links, inline images/video's, etc. As a bonus, it is fairly easy to make it work with org-roam.
*** Configuring logseq
Logseq has an app for [[https://apps.apple.com/us/app/logseq/id1601013908?platform=ipad][iOS]] and [[https://github.com/logseq/logseq/releases/tag/nightly][Android]] (beta). These will use some shared or local storage on your device to access the notes. For me, I store my notes on iCloud Drive, which is available on both my Mac and my iPhone.

Once the org-roam files have been moved to the correct place, we need to setup logseq. First open the settings:

file:static/images/logseq-settings.png
We need set Preferred file format to ~Org~ and Preferred workflow to ~TODO/DOING~.
[[file:static/images/logseq-setting-editor.png]]

From there we need update some of the advanced settings to work with org-roam.

[[file:static/images/logseq-config-edit.png]]

We are going to add 1 line here.
~:org-mode/insert-file-link? true~

file:static/images/logseq-config-org-file-links.png

*** Configuring org-roam
For the emacs-side config we need to make sure that org-roam follows the same directory structure as logseq. The important part of ~org-roam-capture-templates~ is that new files are created in ~pages/~ and that ~org-roam-dailies-directory~ is ~journals/~. The rest can be customized as you like. This can be done with the config below.
#+begin_src emacs-lisp
  (use-package org-roam
    :custom
    (org-roam-directory "<path to logseq root>")
    (org-roam-dailies-directory "journals/")
    (org-roam-capture-templates
     '(("d" "default" plain
        "%?" :target
        (file+head "pages/${slug}.org" "#+title: ${title}\n")
        :unnarrowed t))))
#+end_src

One other issue is that when logseq creates a link, it will do so using a file link. But when org-roam creates a link it will do using a id link. Org-roam doesn't see file links as backlinks and logseq [[https://github.com/logseq/logseq/issues/3281#issuecomment-1059862531][doesn't see id links as backlinks]]. It's kind of a [[https://seuss.fandom.com/wiki/Sleeping_Moose][moose juice]] and goose juice situation. To fix this, there is this [[https://gist.github.com/zot/ddf1a89a567fea73bc3c8a209d48f527][awesome bit of elisp]] from Bill Burdick that will turn notes you created with logseq into org-roam nodes. I highly recommend using that.
*** Taking org-roam on the go
With org-roam and logseq setup, I can now access my notes anywhere. When I am around my computer I have Emacs open and take notes in org-roam. When I am out on the go, I have logseq. I finally have access to my notes everywhere and don't have to be tied to my PC. They are a perfect match.
*** Screenshot
[[file:static/images/logseq-screen-shot.png]]

** DONE [#B] Implementing a safe garbage collector in Rust :rust:
:PROPERTIES:
:EXPORT_DATE: 2022-04-11
:EXPORT_FILE_NAME: implementing-a-safe-garbage-collector-in-rust
:END:
In my [[https://coredumped.dev/2021/10/21/building-an-emacs-lisp-vm-in-rust/][last post]] I introduced an Emacs Lisp VM I was [[https://github.com/CeleritasCelery/rune][writing in Rust]]. My stated goal at the time was to complete a garbage collector. I think Rust has some really interesting properties that will make building garbage collectors easier and safer. Many of the techniques used in my GC are not original and have been developed by other Rustaceans in previous projects.
Updated: 2022-09-06
*** Why use garbage collection?
Virtually all non-trivial programs need some way to reuse memory. Rust does this by tracking every allocation statically to determine when it's no longer in use. However, this system is not flexible enough for some applications. In these cases Rust gives you [[https://doc.rust-lang.org/std/rc/struct.Rc.html][Rc]], the reference counting cell. This cell tracks the number of owners of a piece of memory at runtime. Reference counting has the advantage that is relatively easy to implement and integrates seamlessly with non-rc code. However, it also has two big downsides: It's slower[fn:5] and it can't detect cyclic data (which lisp is full of). For these reasons (and others) dynamic languages often use garbage collection (GC) to manage data.

*** Why is GC hard?
In his book /Crafting Interpreters/ Robert Nystrom has a whole [[http://craftinginterpreters.com/garbage-collection.html#garbage-collection-bugs][section]] dedicated to some of the "nasty bugs" you can have in a garbage collector. The problem lies in identifying all objects that are accessible in the heap. Once you have an object you know is live, it's fairly easy to trace through anything it points to and find other live data. But how do you find the pointers that don't have anything pointing to them? These pointers are problematically scattered across the stack or stored in machine registers. If you miss even one you open yourself up to memory unsafety.
**** How does Emacs solve this problem?
Emacs (and many C based GC implementations) solves this by recognizing that the stack is just a block of memory[fn:6]. If an object can't be reached from the stack, it can't be reached at all. So when garbage collection is triggered, they will dump all registers to the stack and scan the it for anything that "looks like" a pointer. I say looks like because we can't /actually/ know if something is a pointer or a number in the range of a pointer. There is no type information in the hardware. So anything that might be a pointer is treated as a pointer and traced. However because we aren't sure, we can't move any of the gc data. In my implementation we are building a "precise" collector that knows exactly what's a pointer and what's not. That rules out blind stack scanning.
**** Let's start from the beginning
When we allocate a new object, we know that it is unaliased (nothing has a pointer to it). But as soon as we give that pointer to user code, it becomes exposed. Problem is, we need to know when it safe to call ~drop~ and release the memory.  In C, it is up to the user to call ~free~ when they are done with it. But Rust tracks the liveness of the data with the type system. The Rust rule is this: There can be many immutable references to an object or one mutable reference (but not both). If you have immutable references, they get invalidated as soon as a mutable reference is used.

**** Affine types
This key property of Rust (called affine types) is what is used in the gc library [[https://docs.rs/josephine/latest/josephine/][Jospehine]]. They use Rust's borrow checker to ensure no references are live after collection. We do the same. All pointers into the GC heap are borrowed from our allocator (called ~Context~) via an immutable reference. When we call ~garbage_collect~, we take a ~&mut Context~, ensuring that all heap references are no longer accessible.
#+begin_src rust
  let cx: &'ob Context = ...;
  let object: Object<'ob> = cx.add("foo");
  use_object(object);
  cx.garbage_collect(); // Object is no longer accessible
#+end_src

However, if we invalidate all references to the GC heap when we call ~garbage_collect~, we can't access our data at all afterwards. We obviously need something more here.
*** Rooting
What we really want is to have some pointers /preserved/ across calls to ~garbage_collect~. But we need to make sure the gc knows about these special pointers, or it will free the data they point to. We call these special pointers roots.

We have a similar problem with normal data structures. We need to get a reference to a value after we mutate something. How do we solve this problem in that case? Take the example below:
#+begin_src rust
  let mut map = HashMap::new();
  let key = "my key";
  map.insert(key, 13); // insert at key
  let value1 = map.get(key).unwrap(); // get a reference to our item
  let _ = &mut map; // take a mutable reference, invalidating our value1
  let value2 = map.get(key).unwrap(); // Use key to get our data back again
#+end_src

Here we are storing our data inside the map and using some unique token (the key) to keep track of our value inside the data structure when we lose access to our reference. We can do the same thing with our gc ~Context~. We store the roots inside before we garbage collect.
#+begin_src rust
  struct Context {
      roots: HashMap<Token, Object>,
      ...
  }
#+end_src
However we have at least two problems with this:

First, what do we use for a token? Everytime we need to root something we need a token that is unique. Even if we generated something random there is still a chance we could have two roots with the same ~Token~, which would lead to memory unsafety.

Which leads to the second problem: once something is no longer rooted, how do we remove it from the ~Context~? We could require the user to manually call ~remove~ when they no longer need a root, but any failure to do so would result in leaking memory. That is not a good API.
**** Standing on the shoulders of boats
Thankfully, I am not the first person to think about this problem. Saoirse has a [[https://without.boats/blog/shifgrethor-iii/][blog post]] about some novel observations on rooting in Rust. This was implemented in his gc library [[https://github.com/withoutboats/shifgrethor][shifgrethor]]. I will summarize this approach below.

The first observation is that if you don't drop or move a type on the stack, then its lifetime is perfectly stack-like. Shocking I know, but the really cool part about this is that we can use it to define the way we store the roots in ~Context~. What if instead of storing them as a map, we store them as a stack instead? When something is rooted, it is pushed on the stack. When it drops, it is popped from the stack. This also solves our problem of creating a unique ~Token~ to find our object, because when we drop we know that our item will always be the top of the root stack. So no ~Token~ needed.

In order for this to work we have to make sure the object can't move. This sounds just like the pinning! We define a new macro ~root!~ that works similarly to [[https://docs.rs/pin-utils/0.1.0/pin_utils/macro.pin_mut.html][pin_mut!]]. This ensures that our objects behave in a stack-like manner, greatly simplifying the implementation.

As far as keeping our references around post garbage collection, we know that so long as our object is rooted it will be valid. We can keep a reference into the Gc heap until we unroot (i.e. the root goes out of scope). Our ~root!~ macro will change our reference from borrowing from ~Context~ to borrowing from the root. So long as the root is live, our reference is valid; Even if we garbage collect.
#+begin_src rust
  let object = cx.add("new");
  // add the object to the root stack, enabling it to live past collection
  root!(object, cx);
  cx.garbage_collect(); // gc will mark the object as live
  println!("{object}"); // Object is still accessible
#+end_src
**** Returning from functions
There is one more ergonomic problem we want to solve here. Suppose we have the function below:
#+begin_src rust
  fn foo<'a>(cx: &'a mut Context) -> Object<'a> {
      ...
      cx.add(5);
  }
#+end_src
The function takes a ~&mut Context~, and at the end it returns an ~Object~ with the same lifetime. [[https://github.com/pretzelhammer/rust-blog/blob/master/posts/common-rust-lifetime-misconceptions.md#9-downgrading-mut-refs-to-shared-refs-is-safe][Seems fine]] right? Not so. The Rust lifetime rules [[https://doc.rust-lang.org/nomicon/lifetime-mismatch.html][require]] that the /mutable borrow/ of ~Context~ now lasts for the lifetime ~'a~! This means we can't reuse ~Context~ while the ~Object~ is borrowing from it. We could just ~root!~ the object, but that adds overhead to every call. In my interpreter, nearly every method takes ~&mut Context~, so that would get expensive fast.

To work around this we create a new macro ~rebind!~
#+begin_src rust
  rebind!(object, cx);
#+end_src
This macro releases the /mutable/ borrow and reborrows the object with an /immutable/ borrow. This frees ~Context~ to be used by other code while still being [[https://github.com/CeleritasCelery/rune/issues/2][sound]].
**** Preventing escapes
This approach works fine for rooting a single object, but what if we have a whole collection of objects? You might be tempted to think that would be an non-issue, but consider the problem below:
#+begin_src rust
  let rooted: &mut Vec<Object<'root>> = ...; // we have a vec of root objects
  let object: Object<'cx> = cx.add("new"); // object is bound to cx
  rooted.push(object); // object is now bound to rooted
  cx.garbage_collect(); // Object is marked as live and not freed

  // Object is no longer rooted, but still bound to the root lifetime
  let escape: Object<'root> = rooted.pop().unwrap();
  cx.garbage_collect(); // Object is freed
  println!("{escape}"); // Oh No! Use after Free!
#+end_src
We cannot move references out without some way of making sure they stay rooted. Thankfully shifgrethor comes to the rescue here again with its ~Gc~ type.

Once again we can model after the ~pin~ API, since we are trying to solve a similar problem. If you have a ~Pin<P>~ you know that the data point to by ~P~ will not move. Similarly, we create a ~Root<T>~ type that guarantees ~T~ will not move *and* it's rooted. We use the ~struct_root!~ macro to take a data structure ~T~ and return a ~&mut Root<T>~ to it.
#+begin_src rust
  let cx: Context = ...;
  struct_root!(my_struct, cx);
  let _: &mut Root<Vec<Object>> = my_struct;
  // get a reference to vec from root
  let len = my_struct.len();
  // use a special function to mutate
  my_struct.root_push(object);
  // use projection
  let slice: &[Root<Object>] = my_struct.as_slice();
  // Object with lifetime bound to root
  let object: Object<'_> = slice[0].as_obj();
#+end_src

With this API, we can safely get a ~&T~ out when we need to. But mutating the ~T~ requires unsafe methods (like [[https://doc.rust-lang.org/std/pin/struct.Pin.html#method.map_unchecked_mut][map_unchecked_mut]]) to ensure we don't expose roots as in the example above. Using a similar approach to [[https://doc.rust-lang.org/std/pin/index.html#projections-and-structural-pinning][pin projection]] you can get a ~Root~ to the fields of a rooted struct. For example if you have a ~&Root<(T, U)>~ it is safe get a ~&Root<T>~ or ~&Root<U>~. For some the std lib types (vec, hashmap, option, etc) I have already implemented some safe mutation methods like ~push~. If you have a structure that is just built out of these stdlib data structures, you could use a proc macro to derive the "root projection" methods for it.

*** The problem with aliasing
There is still one subtle problem here. You see, we now have a ~&mut Root<T>~, and when we garbage collect, we will trace through ~T~ with ~&T~. However ~&mut T~ guarantees that that it is unique. To break this invariant means undefined behavior. Shifgrethor does not handle this, instead requiring that all roots be immutable (even forbidding interior mutability). Ugh.

How about we use ~UnsafeCell~? It is full of dark magic that lets us do thing we couldn't normally do.

 /* reads documentation */
#+begin_quote
Note that only the immutability guarantee for shared references is affected by UnsafeCell. The uniqueness guarantee for mutable references is unaffected. There is no legal way to obtain aliasing &mut, not even with UnsafeCell<T>.
#+end_quote
Oh, biscuits. What other options do we have? I am sure Rust has an ~AliasCell~ that let's us work around this, right?

 /* googles frantically */

Nope. Though apparently we're not the only ones who need this. The std lib created a [[https://github.com/rust-lang/rust/pull/82834][hack]] to avoid miscompilations when aliasing mutable references, which is used in [[https://github.com/tokio-rs/tokio/pull/3654][Tokio]] as well. We could take that route (and I was really tempted to) but let's see if there is another way we could fix this.
**** A level of indirection
The real problem is that we can't have ~&T~ and ~&mut T~ pointing to the same location in memory. So what if we have them point to different locations? We have the real data in memory, and then the ~Root~ type just has a pointer to it instead of wrapping it.
#+begin_src rust
  pub(crate) struct Root<'a, T> {
      data: *mut T,
      safety: PhantomData<&'a ()>,
  }
#+end_src
Holding a ~&mut Root<T>~ does not alias with ~T~, which lets the garbage collector do its work without undefined behavior. How do we actually get at the ~T~ though? We can define a new wrapper type ~Rt~ (similar to ~Rc~) which lets us access ~T~ under the following conditions:

1. We can borrow ~&Rt~ from a ~&Root~  at any time. There is no unsoundness here. In fact we will just implement ~Deref~ to make this easier.
2. We can borrow ~&mut Rt~ from a ~&mut Root~ if we have a ~&Context~. This ensures that we can never call garbage collect while our mutable reference is live, because garbage collect requires a mutable borrow of ~Context~!
#+begin_src rust
  impl<T> Root<'_, T> {
      fn as_mut<'a>(&'a mut self, _cx: &'a Context) -> &'a mut Rt<T> {
          unsafe { self.deref_mut_unchecked() }
      }
  }
#+end_src
*** A Safe GC
So there you have it! A safe, precise, garbage collector in stable Rust! Now, this comes with a few caveats. It is often said that solving a general problem is three times harder then solving a specific problem. I am solving the specific problem here; creating a GC for my VM. It's not ready to ship as a general purpose library without more work. But I am confident it could be made into a library if needed. Right now the garbage collector is about as naive as possible. But future changes will be under-the-hood improvements that don't change the API.

What I think is really cool is that the API is *safe*! You can't create this in C or C++; The type system is not powerful enough. Rust enables us to have "fearless garbage collection", and no longer be scared of the "nasty bugs" that we might create. As an anecdote, I was pleasantly surprised to find that when I turned on reclaiming memory in my gc, everything just worked first time; No memory leaks, no use-after-free. The API just took care of it at compile time. Miri was satisfied as well.

Overall, I am pretty happy with how it turned out. That being said, there is *a lot* of unsafe code behind the scenes. I am the only person that has reviewed it, and I am not smart enough to get everything right. I created a [[https://github.com/CeleritasCelery/rune/labels/unsound%3F][unsound?]] Label on Github that tracks some of the code I have the least confidence in. If you are initiated in the dark arts of the [[https://doc.rust-lang.org/nomicon/][nomicon]], I would love for you to [[https://github.com/CeleritasCelery/rune/issues?q=is%3Aissue+label%3Aunsound%3F+][prove me wrong]].

I am going to continue work on bootstrapping more elisp files to eventually bootstrap the elisp byte compiler and use that to test my VM. I was forced to take break from that effort and implement the garbage collector because I kept using too much memory! Implementing the garbage collector was a much bigger rabbit hole than I expected. Hopefully this will help move the community forward on the quest for a Rust GC.

****  Have a comment?
View the discussion on [[https://www.reddit.com/r/rust/comments/u21w97/implementing_a_safe_garbage_collector_in_rust/][Reddit]] or [[https://news.ycombinator.com/item?id=31166368][Hacker News]], send me an [[mailto:troy.hinckley@dabrev.com][email]], or open an [[https://github.com/CeleritasCelery/rune/issues/new][issue]].

[fn:5] Why is reference counting slower then garbage collection? There is a lot that goes into it, but it boils down to two main issues:
1. Every time you copy a ~Rc~ pointer you need to update the reference count. This involves reading the memory, updating the count, and writing it back. Compare that to an "normal" pointer copy where you don't need to even /access/ the memory at all. GC's do have to trace the memory eventually, but this overhead can be moved to a time when it will have less impact (or even moved to another thread). RC overhead needs to happen real time, and happens /every time/.
2. RC can fall victim to "destructor avalanche" when the root of a chain of objects goes out of scope.  This results in unbounded pause times. Modern GC's by contrast are usually incremental, and will do work in small chunks to preventing long pauses.

With all of these issues, there are techniques to try and mitigate them and get some performance back. But even a naive GC can often beat a well optimized RC implementation. And optimized GC (like JVM or V8) will always outclass reference counting. See [[https://softwareengineering.stackexchange.com/questions/30254/why-garbage-collection-if-smart-pointers-are-there][this SO post]] and [[https://web.archive.org/web/20200325094430/http://flyingfrogblog.blogspot.com/2010/12/why-gc-when-you-have-reference-counted.html][follow up post]] for more.
[fn:6] I don't think this is true in Rust though. My best guess is that scanning the stack would violate some of Rust's aliasing rules and be UB.

** DONE [#B] A vision of a multi-threaded Emacs :emacs:
:PROPERTIES:
:EXPORT_DATE: 2022-05-19
:EXPORT_FILE_NAME: a-vision-of-a-multi-threaded-emacs
:END:
*** The Threading library
Starting in Emacs 26 some very ambitious changes were added. Basic thread support was enabled, laying the groundwork for a future concurrent emacs. The [[https://www.gnu.org/software/emacs/manual/html_node/elisp/Threads.html][docs]] layout this possibility:

#+begin_quote
Emacs Lisp provides a limited form of concurrency, called threads. All the threads in a given instance of Emacs share the same memory. Concurrency in Emacs Lisp is “mostly cooperative”, meaning that Emacs will only switch execution between threads at well-defined times. However, the Emacs thread support has been designed in a way to later allow more fine-grained concurrency
#+end_quote

What would a future with fine-grained concurrency look like? Could we have an Emacs that uses more then 1 thread? This post sketches out some rough ideas of what that could look like from an elisp perspective. I am going to take the easy way out and completely ignore  /how/ to actually implement this, just speculating on the big what-ifs.

*** Concurrency vs parallelism
The ~thread~ feature is specifically trying to enable /concurrency/, which is the ability to interweave lines of execution to make progress on more then one program at a time. This is the model used by async/await and coroutines. Concurrency is useful when your application is IO bound. The library enables you to switch between concurrent programs at designated points, which is why it is call /cooperative/ concurrency. /Parallelism/ on the other hand is the ability to actually run multiple programs at the same time. This is useful when your application is CPU bound. See [[https://oxylabs.io/blog/concurrency-vs-parallelism][this post]] for a more detailed explanation.

There are two main scenarios where concurrency/parallelism are useful.
1. When I want some elisp task done in the background without blocking my main user thread, I can use concurrency. This includes things like handling filter output, indexing buffers, watching for changes, etc. The background task will "steal" idle time from my main thread to make incremental progress on its job.
2. When I want to get the results of some task faster, I need parallelism. This includes things like updating or searching buffers, applying font lock, or loading code. In order to do these task faster I need multiple threads running at the same time.

Note that parallelism can service both use case 1 and 2, but concurrency can only deal with use case 1. In some sense, parallelism is a superset of concurrency. All parallel code is concurrent, but concurrent code is not necessarily parallel. For this reason, I am much more interested in a parallel Emacs then just a concurrent one.

*** What level of parallelism do you want?
My oversimplification of parallel languages breaks them into three categories:

- Level 1 - memory unsafe and data races allowed  :: Languages where incorrect code can lead to corruption of the program state and segfaults. This includes C++ and Swift.
- Level 2 - memory safe and data races allowed :: Languages where parallelism is memory safe, but can still lead to data races. This includes Java and Go.
- Level 3 - memory safe and no data races :: Languages that enable [[https://doc.rust-lang.org/book/ch16-00-concurrency.html][fearless concurrency]] by eliminating unguarded access to shared-memory. This includes Clojure, Rust, and TCL.

Generally the closer you are Level 1 the more footguns there are, but the more performance you can squeeze out. The higher you go the easier concurrent code is to write, but you have less performance and control. The exception to this is Rust, which is a safe Level 3 language with the performance of a Level 1.

So where do we want Emacs to land on this spectrum? The creator of [[https://github.com/colesbury/nogil][nogil]] python, a multi-threaded python implementation, [[https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0/edit][said this]]:

#+begin_quote
The project aims for a concurrency model that matches the threads + shared memory model implemented in common operating systems... The shared-memory model is notoriously difficult to program correctly, but provides a good base to build better abstractions because it closely matches the primitives provided by the operating system (e.g. threads, mutexes).
#+end_quote

The argument is that Level 2 is the right balance, because you avoid crazy bugs you get with unsafe languages but still have more flexibility then level 3. You should just give programmers the tools they need to build safe abstractions.

I, however, disagree with that take. As the author said, shared memory is "notoriously difficult" to do correctly. As Emacs pulls in hundreds of packages, the potential for data races grows exponentially. Even Emacs' current threads library [[https://nullprogram.com/blog/2018/05/31/][suffers from data races]] which is one of the reasons I believe it has not seen much adoption. We need to make concurrency as pain free as possible if it is going to be usable. Therefore I am in the "Emacs parallelism should be level 3" boat.

**** What are some requirements for parallel Emacs
So what are the standards we want for a multi-threaded Emacs implementation? Here is my short list:

1. No [[https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/][function coloring]] or special requirements on functions. One of Emacs' big advantages is the huge bulk of existing lisp code. We want to reuse as much as we can. This is generally only a problem with concurrent schemes like async/wait.
2. No data races. This will make programs significantly easier to write correctly, but is also going to make our code more limited (** /foreshadowing **/).
3. We want the behavior of multi-threaded code to be as close to single-threaded code as possible. More on this later.

*** A jumping off point
Most people have never heard of TCL (or if they have they've never used it) but I find it has a [[https://www.activestate.com/blog/threads-done-right-tcl/][very simple approach]] to multi-threading. Essentially the interpreter can work in its own thread, and carries with it all of its state. This is the multi-interpreter approach; Every thread starts in a clean environment with its own interpreter. "Messages" can be passed to any thread and they can return a result. In elisp it could look something like this.

#+begin_src emacs-lisp
  (let ((thread1 (make-thread))
        (thread2 (make-thread))
        (filename "file.txt")
        result1 result2)
    ;; Send commands to the threads
    (thread-send thread1 #'setup-function)
    (setq result1 (thread-send thread1 #'my-function))
    ;; closure captures a copy of the variable
    (setq result2 (thread-send thread2 (lambda () (delete-file filename))))
    ;; run some other code while threads are working
    (my-other-function)
    ;; wait for the results to populate
    (thread-wait result1 result2)
    ;; return the results in list
    (list result1 result2))
#+end_src

This is really simple and really effective, but it has some limitations. First is that since each thread starts a new interpreter, you need to load a bunch of lisp code to do almost anything useful. This means that thread overhead is significant and is not a good fit for small tasks. Second, since you are copying objects between threads when you pass them with ~thread-send~ you can't modifying existing buffers[fn:3]. And buffers are probably the most important use case here. Let's see if we can fix that.

*** Can you pass the buffer please?
What if instead of needing to copy the buffers between threads, they could be shared? I know, I know, shared-memory is a footgun, but we are going to use mutexes! So it's more like sharing in pre-school where everyone gets a turn. Each buffer is guarded by a mutex, and only one thread can have access to a buffer at a given time. The way you acquire the mutex is by switching to that buffer (using ~set-buffer~ , ~switch-to-buffer~, or ~with-current-buffer~). Just as you can only have a single "current buffer", you can only have the mutex for a single buffer at a time. A thread can switch to a buffer, do some operations, then release it. This is all well and good, but we have a major issue; shared-state.

You see, for a buffer to really be useful you need have the buffer local variables. Without those you can't even know the ~major-mode~! But buffer local variable can share data with global variables, and each thread has its own set of globals. Consider the code below:
#+begin_src emacs-lisp
  (defvar local nil)
  (defvar global nil)
  (make-variable-buffer-local 'local)
  (setq local '(1 2 3))
  (setq global local)
#+end_src

Here both ~local~ and ~global~ share the same cons cells. If I mutate one it will mutate the other. This obviously won't work if I am sharing buffer local variables between threads. What we need to sever the ties between these. Buffer local variables can't share any data with non-buffer local variables. You could setup a write barrier that would make copies when a thread releases the mutex. But I am not going to get into the /how/ (I get to ignore implementation details remember?).

This would technically make multi-threaded emacs have different behavior then the old single threaded one (I told you we would talk about that later). But I would argue that if you are relying on sharing data between globals and buffer-locals for the correct operation of your code, it is in serious need of a refactor. I imagine that in real life this situation is very rare.

*** Cutting down initialization
As it currently stands you basically need to load your entire init file in each new thread. Why can't you just load the bare minimum elisp? Consider what would happen when a buffer local hook calls some function from another package? We need to make sure the code is loaded, and the only way we can do that is by loading the init file which contains all package initialization. There is also the problem that the state can get out sync. Each thread would start in a clean state, but this is not going to match the state of your main thread. This impedance mismatch is a clear source of bugs.

What if we did something crazy? What if we said that all functions are shared between threads? If you think about it, this is almost a perfect match. Function rarely change, and when they do, you can just replace the whole function atomically. However to do this you would need to address the /functional literal mutation problem/ I talk about [[https://coredumped.dev/2021/04/07/when-pure-function-lie/][here]]. Otherwise you could have multiple threads mutating the same function constants and potentially corrupting the VM state. But again, /implementation details/.

Okay so that takes care of functions, but what about variables? Sharing variables would lead to data races, which is exactly what we are trying avoid. What if instead of sharing, we copied variables on demand? Hear me out! The first time a "sub-thread" does a lookup of a global variable, its value is copied from the main thread. This sets the initial value in that thread. From then on that copy of the value is "owned" by the thread and it can be mutated or read whenever. This would also help with the out-of-sync with the main thread problem we mentioned earlier. The state in your sub-thread would be much closer to the main global state. There could be a message queue internal to the VM that sends these requests back and forth. At certain points, the main thread would check the queue and send the values.

But this also means that sub-threads could spend a decent amount of time waiting for the main thread to be ready to send the values it needs, at least at the start. There could be a couple ways to alleviate this. The one that comes to mind is that you could cache the list of variables used at the call site of the thread creation. Then next time the thread is called you eagerly copy all the variables it had used before. This would make repeated initialization of the same thread much faster, but could also mean you get different variables the first time vs following times (if the calling function changed something immediately afterwards for example). As with everything; trade-offs.

There is one big reason for all this song and dance around buffers locals, functions, and variables: reusing existing elisp code. Languages that were created with concurrency in mind have designed their languages around these considerations. But Emacs is a giant ball of mutable state. Taming that to do something useful in a multi-threaded world while still reusing existing code is tricky.
*** Going Green
So our threads are pretty cheap to create (as threads go), but not to keep around. Each elisp thread maps to an OS thread, and even when thread is idle it is still taking OS resources. Go and Clojure have solved this by creating so called green threads that are managed by the runtime. The green threads will be executed on OS threads, but they can be created, destroyed and managed by the VM. In Go you can create thousands of green threads and it will not impact the system. Don't try at home with OS threads.

Now that we have green threads, the observant among you will notice that we have basically reinvented goroutines. All we need to do is add channels and we can have something close to [[https://clojuredocs.org/clojure.core.async][core.async]]. I imagine usage looking something like this:

#+begin_src emacs-lisp
  ;; Delete 3 files concurrently
  (let ((files '("file1.txt" "file2.txt" "file3.txt")))
    (dolist (file files)
      (go (delete-file file))))

  ;; loop through all buffers and insert "a" for some reason...
  (let* ((c (chan)))
    (go (loop-chan chan buffer
                   (with-current-buffer buffer
                     (insert "a"))))
    (dolist (buffer (buffer-list))
      (chan-put c buffer)))

  ;; run a buffer search on another thread and yield the output
  (let ((queries (chan))
        (results (chan)))
    (go (let* ((message (channel-take queries))
               (buffer (car message))
               (string (cdr message)))
          (chan-put results
                    (with-current-buffer buffer
                      (save-excursion
                        (re-search-forward string)
                        (buffer-substring
                         (point)
                         (+ 5 (point))))))))
    (chan-put queries (cons other-buffer "foo"))
    (do-something-else)
    ;; wait till the routine returns
    (setq my-string (chan-take results)))
#+end_src

*** So where does that leave us?
We have created some simple light weight green threads for Emacs. Well, we didn't actually create anything, but we sure did talk about it a lot! The thing I like about this approach is that threads are easy to create and use to accomplish work in parallel. There are no data races and footguns have been minimized. But I can still see a few open problems that are not solved:

1. The most important buffer in Emacs is the one you are currently editing. But with the mutex scheme, you can't use any other threads to work on that buffer! If you want to index or search or syntax-highlight the buffer, that still needs to use your main thread, meaning that the user is blocked. I don't like it, but am not sure of a clean way to fix it.
2. What about when you need to iterate over all buffers (like with ~ibuffer~)? Here you would need to acquire the mutex for each one in turn. If another thread is using a buffer the main thread will have to wait. Hopefully sub-threads would choose to do their work incrementally, giving time for the thread to yield the mutex.
3. Since the sub-threads take their global variables from the main thread, you can't load code in parallel. Only the main thread can load code that can be used by everyone.
4. I haven't even mentioned a bunch of other multi-threading concerns like cancellation, atomics, garbage collection, message queue buffering, semaphores, signals, error reporting, debugging, concurrent data structures, C integration, deadlock, and livelock to name a few. Perhaps those are a topic for a future post.

It has been fun to speculate about multi-threaded Emacs. But the real question is, would it be worth it? Emacs has gotten along just fine with a single thread; In fact many (most?) dynamic languages have. I would guess that threads would only be useful in about 10% of the programming tasks you would do in elisp. But when threads can be used, they would be big boon.

As with everything in engineering, concurrency comes with trade-offs. Implementing a scheme like I described would be a monumental task. It would probably involve a complete rewrite of the core runtime[fn:1]. Also anytime you make an interpreter multi-threaded, you make single threaded code slower[fn:2]. There is no avoiding that. If 90% of code is still single-threaded, is that worth the cost?

Anyways, I would love to get some feedback on the ideas presented. Are there obvious holes that I missed? Would this scheme be useful? Do you know way that these could be implemented (or would not be possible to implement)? How does this compare to other dynamic languages? Do you prefer the more "thread-like" or "green-thread-like" approach? Is there a way to address some of the problems presented above?
**** Have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/utzxir/a_vision_of_a_multithreaded_emacs/][Reddit]], [[https://news.ycombinator.com/item?id=31559818][Hacker News]], or send me an [[mailto:troy.hinckley@dabrev.com][email]]

[fn:1] Similar to what is being attempted for python [[https://github.com/colesbury/nogil][here]] and [[https://github.com/larryhastings/gilectoy][here]].

[fn:2] Why does adding multi-threading make single-threaded code slower? It essentially comes down the fact that synchronizing memory between cores is [[https://spcl.inf.ethz.ch/Publications/.pdf/atomic-bench.pdf][slow]]. So unless your interpreters are completely independent (which would not be very useful), you are going to add some overhead. Now this doesn't have to be a lot, it can be just a couple percentage points. But it will always be something. The [[https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0/edit][nogil design doc]] goes into great detail about some of performance implications (for python) and how to mitigate them. See also this [[https://github.com/ocamllabs/compiler-hacking/wiki/OCaml-Multicore-Runtime][design doc]] for multi-core OCaml or [[https://www.atdot.net/~ko1/activities/2016_rubykaigi.pdf][this presentation]] on the Actor model in Ruby.

[fn:3] And you can't share anything that doesn't have a clear [[https://www.gnu.org/software/emacs/manual/html_node/elisp/Streams-Intro.html][readable]] representation such as windows, frames, or makers.
** DONE [#B] Design of Emacs in Rust :rust:emacs:
:PROPERTIES:
:EXPORT_DATE: 2023-01-17
:EXPORT_FILE_NAME: design-of-emacs-in-rust
:END:
This is the third post in my series about writing an [[https://coredumped.dev/2023/01/17/design-of-emacs-in-rust/][Emacs core in Rust]]. The [[https://coredumped.dev/2021/10/21/building-an-emacs-lisp-vm-in-rust/][first post]] laid out my initial observations and ideas about the language runtime. The [[https://coredumped.dev/2022/04/11/implementing-a-safe-garbage-collector-in-rust/][second post]] focused on building a safe garbage collector in Rust using the type system. I initially stated that I wanted to reach the point where I could bootstrap bytecomp.el (the elisp byte compiler). That goal is reached[fn:1], so I am providing an update on my latest learnings.
*** Interpreter
Emacs has multiple runtime environments including the interpreter, bytecompiler, and native code. My initial goal was to not implement more then one runtime, so I determined to only have a bytecode VM. I created a bootstrap compiler to get [[https://github.com/emacs-mirror/emacs/blob/master/lisp/emacs-lisp/bytecomp.el][bytecomp.el]] enabled and then planned to use the elisp compiler for the rest of the code. Despite some good first steps I continually ran into problems with the bootstrap process[fn:4]. I would hack the elisp or the load order to try and work around the problem, but eventually it became a game of whack-a-mole. I spent most of my time trying to workaround issues instead of actually writing code.

I finally broke down and implemented an elisp interpreter. I agree with Stefan Monnier that the the elisp interpreter [[https://lists.gnu.org/archive/html/emacs-devel/2021-03/msg00068.html][is a crutch to bootstrap the system]], but the startup code is written in a way that you can't bootstrap without the crutch[fn:2]. Now I have both an interpreter and bytecode. I learned that I was a little too ambitious in my desire to get rid of the interpreter since it is much easier to debug than bytecode is.

*** Symbol Layout
I learned some interesting details about how Emacs represents symbols in objects. The symbol itself is a [[https://github.com/emacs-mirror/emacs/blob/master/src/lisp.h#L833-L887][struct]] that hold a bunch of fields like function, value, or properties. You would assume that a ~LispObject~ that contains a symbol would be a pointer to that struct, and indeed it was for a long time.

The issue with using a pointer comes from codegen. On most architectures immediate values larger than a certain size (usually 16 bits) need to be moved into a register. Pointers are generally larger than that, so they can't be embedded directly in the instruction encoding. Instead they have to be loaded with a separate instruction. Total code size will also be increase because there need to be pointer sized constants all over the code.

To address this, the Emacs maintainers implemented a clever scheme where symbols are not encoded as pointers; they are offsets from a static symbol array called ~lispsym~. This means the first object in the array has offset 0, the next one has offset ~sizeof(LispSymbol)~, and so forth. When you want to get the pointer, you add the [[https://github.com/emacs-mirror/emacs/blob/master/src/lisp.h#L1142-L1144][start address of lispsym to the offset]]. The symbol at index 0 of ~lispsym~ is the symbol ~nil~, which is used more than any other[fn:3]. As reported in the Emacs [[https://github.com/emacs-mirror/emacs/blob/master/src/ChangeLog.13#L1653-L1662][changelog]], this approach reduced the code size by 2.5% and led to a minor speedup.
**** Implementation in Rust
One of the really nice things about this scheme is that it maps well to Rust. The language has a limitation that constants cannot refer to statics. The [[https://github.com/rust-lang/const-eval/issues/11][reasons for this]] are complex, and I hope this will be changed someday. It's very useful for symbols to be constants because then they can be used in match patterns and const functions. My first attempt working around this limitation was a terrible hack involving self-referential functions and other terrible ideas. Once I learned the Emacs approach, things became super clean and simple:

#+begin_src rust
  pub(crate) struct Symbol<'a> {
      // Offset from the start of the symbol table
      data: *const SymbolCell,
      marker: PhantomData<&'a SymbolCell>,
  }

  // Can be used in match statements ...
  match obj {
      Object::Symbol(NIL) => {...},
      ...
  }

  // Or even simpler with associated constants
  match obj {
      Object::NIL => {...},
      ...
  }
#+end_src

*** Representing Strings
[[https://en.wikipedia.org/wiki/UTF-8#Encoding][UTF-8]] has become the de facto standard for representing text. Emacs closely follows the unicode standard, but uses an extended version of UTF-8 which enables support for raw bytes. Let me explain.

One of the reasons that UTF-8 is so useful is because [[https://www.asciitable.com/][ASCII characters]] are automatically valid. These are the values between 0 and 127 and includes the English alphabet. If you assigned a code point to every value of the byte you could only have 256 possible characters. Instead, bigger code points are encoded using multiple bytes. The values above 127 are reserved for leading bytes in UTF-8. Thus a random value above the ASCII range may not be valid. However Emacs [[https://www.gnu.org/software/emacs/manual/html_node/elisp/Text-Representations.html][extends unicode]] to reserve the code points [[https://github.com/emacs-mirror/emacs/blob/master/src/character.h#L31-L47][0x3FFF80 to 0x3FFFFF]] as "raw bytes between 128 and 255".

The advantage of this is that Emacs can distinguish a "normal" byte that just happens to be valid UTF-8 from a "raw byte" that is not intended to be valid. However the display representation can be a confused with unprintable characters. For example, if you see this printed representation in the buffer:

~\201~

it can either be the unicode codepoint ~0x81~ (Emacs displays things in octal) or the raw byte ~0x81~ represented by codepoint ~0x3FFF81~. The only way to tell the difference is to inspect the character.

There are other use-cases for a "mostly UTF-8 but not quite" type of formats. For example, [[https://simonsapin.github.io/wtf-8/#generalized-utf-8][WTF-8]] is used to handle invalid UTF-16 conversions to UTF-8. The downside of these formats is that you lose compliance with the spec, which means you can't use third-party string libraries that operate on code points. The Remacs team [[https://github.com/remacs/remacs/blob/master/rust_src/src/multibyte.rs#L13-L33][had to rewrite the primitive string type]] in their project to support raw bytes.

I am taking the [[https://docs.rs/bstr/latest/bstr/][bstr]] approach for my project. That assumes that strings are conventionally UTF-8, but will handle invalid bytes gracefully. Raw bytes are no longer distinguishable from other bytes, but I see that as an acceptable trade-off to use existing libraries.

*** Finding GC Roots
One of the trickiest parts of implementing a garbage collector is handling roots. You need to ensure that any value that is reachable from the program stack or machine register is not garbage collected. In the days of yore, Emacs had a method called GCPRO to handle this. As the [[http://www.sxemacs.org/docs/internals/GCPROing.html#GCPROing][SXEmacs docs say]], "GCPROing is one of the ugliest and trickiest parts of Emacs internals".

In order to use GCPRO,  there were a bunch of rules provided to avoid memory issues including:

1. For every GCPROn, there have to be declarations of struct gcpro gcpro1, gcpro2, etc.
2. You must UNGCPRO anything that’s GCPROed
3. You must not UNGCPRO if you haven’t GCPROed
4. Make sure not to use a relocated string. They are not GCPROed
5. If you have to nest GCPRO’s, use NGCPROn
6. Don't GCPRO uninitialized memory
7. If you create any Lisp objects, you are responsible for GCPROing them
8. Make sure that traps can't occur between allocating memory and GCPRO

The docs state that bugs resulting from not following these rules are "intermittent and extremely difficult to track down, often showing up in crashes inside of garbage-collect or in weirdly corrupted objects or even in incorrect values in a totally different section of code".

It's no wonder that the maintainers decided to abandon this approach and instead use conservative stack scanning (where you treat everything that looks like a pointer on the stack as pointer). This is what the Spidermonkey team [[https://blog.mozilla.org/javascript/2013/07/18/clawing-our-way-back-to-precision/][had to say]] about switching in Firefox:

#+begin_quote
Language implementations with automatic memory management often start out using exact rooting. At some point, all the careful rooting code gets to be so painful to maintain that a conservative scanner is introduced. The embedding API gets dramatically simpler, everyone cheers and breathes a sigh of relief, and work moves on (at a quicker pace!)
#+end_quote

However this comes with a tradeoff, you also lose the ability to precisely know what really is a pointer. This may not seem like a big deal, but it limits the kind of collectors [[https://lists.gnu.org/archive/html/emacs-devel/2015-09/msg00695.html][you can implement]] (such as a copying GC). The same post by the Spidermonkey team mentions their effort to "claw their way back to precision". They needed the performance improvements that can come with precise memory management techniques. Despite that, given all the complexity added to Emacs by GCPRO I think removing it was the right call.

Rust gives us a different option. It's powerful type system and affine types let us have both precision and a bug-free implementation. I wrote a whole post describing how you to [[https://coredumped.dev/2022/04/11/implementing-a-safe-garbage-collector-in-rust/][implement a safe GC in rust]],  so I won't expand on that here. Suffice it to say that the borrow checker can ensure that all stack roots are accounted for.

*** Making Emacs Multi-Threaded
I mentioned in my first post in this series how Rust does not have a concept of single threaded applications. Every program is considered multi-threaded, even if only used with a single thread. This changes how you design programs and data structures, with the biggest difference being no unguarded global mutable state. I implemented my VM to support multi-threading, opening the possibility for the elisp itself to take advantage of that. I [[https://coredumped.dev/2022/05/19/a-vision-of-a-multi-threaded-emacs/][wrote a post]] about my ideas for implementing a multi-core Emacs. That approach is what I am using in this Rust runtime. In fact, the basic support is already there! Right now you can write code like this:
#+begin_src emacs-lisp
(go (lambda () (do-something-in-thread)))
#+end_src

And it will execute the code in another thread! The other thread has access to all the functions defined in the runtime. It won't be too much work to add support for channels and variables to send data back and forth.

*** Future Design Work
I hit my initial goal of bootstrapping the elisp bytecompiler. My new goal is to finish bootstrapping all elisp included with GNU Emacs and bytecompile it. While working on my bytecompiler objective I tried very hard to not get sucked into tangents. However, there are several things that are in desperate need of attention. For one, the [[https://github.com/CeleritasCelery/rune/blob/master/src/core/gc/context.rs][garbage collector]] is the most basic mark-and-sweep imaginable. I am going to bring it up to snuff and implement a [[https://github.com/CeleritasCelery/rune/issues/20][generational copying collector]].

There is also a big need for testing. My code has unit tests, but the true spec is not my tests - it's Emacs itself. I plan to write a test harness that will let me fuzz my implementation against GNU Emacs. This will hopefully help me flush out a bunch of issues instead of hitting them during development. The bigger the core gets, the more important this will be.

This project is still far from complete or useful. It's truly a love letter to Emacs and has been an amazing learning experience. There have been a lot of design challenges trying to do things in a memory-safe and multi-threaded way. I created a [[https://github.com/CeleritasCelery/rune/blob/master/details.org][design doc]] that contains a bunch of loosely structured thoughts on ways different things could be implemented. I also created a bunch of [[https://github.com/CeleritasCelery/rune/issues?q=is%3Aopen+is%3Aissue+label%3A%22design+needed%22][issues]] in the issue tracker with the label: *Design Needed*. This is where I have put some ideas about how to handle things from [[https://github.com/CeleritasCelery/rune/issues/21][multi-threading]] and [[https://github.com/CeleritasCelery/rune/issues/14][string representation]] to [[https://github.com/CeleritasCelery/rune/issues/19][regex]] and [[https://github.com/CeleritasCelery/rune/issues/17][buffer data structures]]. If you have an eye for design or just want to add your thoughts, go ahead and submit a comment or new issue.

**** Have a comment?
Join the [[https://discu.eu/?q=https%3A%2F%2Fcoredumped.dev%2F2023%2F01%2F17%2Fdesign-of-emacs-in-rust%2F%23fnref%3A3&submit_title=Design%20of%20Emacs%20in%20Rust%20%E2%80%A2%20Core%20Dumped][discussion]] or send me an [[mailto:troy.hinckley@dabrev.com][email]]

[fn:1] This would not have been possible without Rocky Bernstein's amazing resource; [[http://rocky.github.io/elisp-bytecode.pdf][Bytecode Reference Manual]].

[fn:4] Most of the boostrap issues were functions being called before their macros were defined. Or it would be variables that were not defined before functions were evaluated. This is fine for an interpreter because the macros and variables would be code paths that were not used, so the macro was never evaluated. But a bytecompiler will expand all macros in a function, so it would generate a function call instead of a macro expansion.

[fn:2] Too be fair, bootstrapping is a really hard problem. You would need something to bootstrap a bytecompiler written in the language you are bytecompiling. My approach was to create a minimal compiler in rust, but you could also take the approach of using bytecode written in a previous build of Emacs. [[https://ziglang.org/news/goodbye-cpp/#the-solution-space][This post by the Zig team]] covers some of the many ways you can bootstrap a language.

[fn:3] Another advantage of ~nil~ being the symbol with offset 0 is that you can now test for nil by comparing with zero, which usually has it's own dedicated instruction.

** TODO Gap buffers
I have been working on a hobby project to reimagine the C core of Emacs in Rust. On this journey I reached the point where I needed some way to represent the text of a buffer. The simplest approach is to just use a large string or array of lines. However these each suffer from poor performance as the either the size or line length of text increases.

GNU Emacs has famously used a gap buffer to represent editable text. It's even mentioned by name on the [[https://en.wikipedia.org/wiki/Gap_buffer][wikipedia]] entry for it. Gap buffers have advantage of allowing fast local edits with a fairly simple design. I see it as analogous to the more general data structure, the "array". Because really a gap buffer is just an array that is optimized for inserting at a "cursor" instead of at the end. Using a gap buffer has served Emacs well over many decades.

Despite this, Emacs seems largely alone in it's choice in the modern world. Most popular editors today use some variation of either a [[https://code.visualstudio.com/blogs/2018/03/23/text-buffer-reimplementation][piece table]] or a [[https://blog.jetbrains.com/fleet/2022/02/fleet-below-deck-part-ii-breaking-down-the-editor/][rope]]. Rather then storing data as large contiguous array, these data structures chop the buffer into small chunks and operate on those. This enables them to avoid the O(N) penalty of moving the cursor when doing edits far away and the latency of resizing the buffer. Chris Wellons has a [[https://nullprogram.com/blog/2017/09/07/][post]] entitled "Gap Buffers Are Not Optimized for Multiple Cursors" giving an intuitive explanation for why gap buffers don't do well for non-localized edits.

So it seemed like the most obvious solution was to pick from one of Rusts [[https://crates.io/search?q=ropey][many rope crates]] and call it a day. But I was not entirely convinced. Modern computers can operate very quickly over linear memory. Despite looking worse on paper, arrays will often beat more complex data structures in the real world, even on operations that they are technically O(N). I wanted to see for myself. So I built a gap buffer and stacked it up against the competition.

*** design
My gap buffer is fairly true to the original design, with one big change; I store metrics about the buffer in a separate tree. These metrics include things like char and line position, but could in theory include anything you want (like UTF-16 [[https://docs.rs/ropey/latest/ropey/struct.Rope.html#method.char_to_utf16_cu][code units]]). This means that /finding/ an arbitrary position in the buffer becomes at worse O(Log N), but we still have to pay the cost of moving the gap.
*** text data structure shoot out
The ropes that I will be comparing against are [[https://docs.rs/ropey/latest/ropey/index.html][ropey]], [[https://docs.rs/crop/0.3.0/crop/index.html][crop]], and [[https://docs.rs/jumprope/latest/jumprope/index.html][jumprope]]. The last one is technically implemented via [[https://en.wikipedia.org/wiki/Skip_list][skip lists]], but that is really an implementation detail, the performance should be similar.

*** overhead
Let's start out with memory overhead. The way to read this is if something has 50% overhead, that means you need 1.5GB of memory to open a 1GB file.

<Overhead figure>

Woah! Jumprope is way outside the norm here, almost doubling the memory needed to open a file[fn:2]. Crop and Ropey and much closer to what you would expect. However this is really the ideal case of opening a new file. Each rope node will be perfectly filled and the tree properly balanced.

**** edit overhead
Let's look at the overhead when edits are applied to the text. These could be things like large search and replace or multiple cursors. This tends to leave the ropes in a less ideal state, and hence have higher overhead.

<Overhead edits>

Fairly significant change for all ropes. Jumprope has gone through the roof, jumping so high that I didn't even bother expanding the plot to show it. Even crop, which had the lowest rope overhead in ideal conditions, has jumped almost 20x. The gap buffer on the other hand has barely moved. Unlike ropes, a gap buffer is always in ideal state with regards to layout.

*** buffer cost
Before we jump into benchmarks comparing ropes and gap buffers. Let's look at the time it takes to complete the two highest latency operations on gap buffers: resizing and moving the gap.

**** resize
Insertion in a gap buffer is /amortized/ O(1), just like an appending to an vector. But also like vectors, they only achieve this in the amortized case. Vectors need to resize once they get full, and this is a O(N) operation. But since the time between resizing is N appends, it averages out to O(1). Gap buffers are similar, except that we don't usually continue to grow the gap as the text get's larger (that would lead to more overhead). In this sense, it isn't truly O(1) insertion time, but even if it was, we generally care about the latency in interactive applications more then we care about average case.

<Resize>

Note that both axis are logarithmic. We can see that the cost to resize grows linearly with the size of the buffer, which is what we would expect. With a 1GB[fn:1] file, it takes about 125ms to resize, which is definitely perceptible.

**** moving the gap

How long does it take to slide the gap a given distance? This delay is added anytime we edit a different part of the buffer then we are currently in.

<Move gap>

Moving the gap 1GB is significantly faster then resizing 1GB, taking only 23ms. This isn't nothing, but is small enough to be imperceptible. Of course since this is a O(N) relationship, moving the gap even farther will have proportionally higher cost.

In practice these latencies will be less of an issue, because giant files tend to be log files and auto generated output, which are rarely editing. But it is still an unavoidable cost of storing data in contiguous fixed-sized structure.

*** creating

**** creating from a String
Let see how long it take to create a new data structure from a ~String~. The string below is 1GB in size.

<Build from String>

Gap buffer is significantly faster then the rest, but it's not really a fair comparison. A ~String~ is essentially already a gap buffer. It has the string text, followed by some amount of unused capacity which can be used as a gap. So in this case we don't need to copy or allocate at all, instead reusing the allocation from the string. That won't always be the case, so how do things compare if we force them all to the copy the source text. This would be the case when loading from a file.

<Build from str>

Forcing a copy makes the gap buffer take about three times as long, but it is still faster then the rope implementations.

**** saving
What about saving a file? Here we are not going to benchmark the actual file system overhead, but instead use "writing the contents to a string" as a proxy.

<Save>

All containers are pretty comparable on this front.

*** real world
To start this off, we have a set of 4 [[https://github.com/josephg/editing-traces][real world]] benchmarks from the author of jumprope. These are recordings or "traces" of actual people editing some text in an editor. They represent a good baseline for performance.

<Plot>

Aside from ropey, all the containers have comparable performance. In all benchmarks but one, the gap buffer is the fastest, but not by any meaningful amount. I also want to emphasize here how crazy fast each of these are. The traces are thousands or tens of thousands of edits long, and even the slowest is still completed in milliseconds. For interactive edits, any container will be blazing fast.

*** multiple cursors
Back in 2017, Chris Wellons wrote a [[https://nullprogram.com/blog/2017/09/07/][blog post]] titled "Gap Buffers Are Not Optimized for Multiple Cursors". It makes the case that since gap buffers have to move the gap buffer across the whole range for each edit, they don't perform well with multiple cursors. [[https://github.com/hauleth/sad.vim#why-not-multiple-cursors][This]] fact [[https://cdacamar.github.io/data%20structures/algorithms/benchmarking/text%20editors/c++/editor-data-structures/][has]] now [[https://github.com/emacs-ng/emacs-ng/issues/378#issuecomment-907577662][become]] part [[https://vms.wwwtech.de/notes/360][of]] Internet [[https://vuink.com/post/ahyycebtenz-d-dpbz/blog/2017/09/07][lore]].

*** worst case

*** search benchmarks

*** what about piece tables?
Gap buffers and ropes are not the only data structures used in text editing. Heavy hitters like Atom and VsCode use a piece table (or [[https://code.visualstudio.com/blogs/2018/03/23/text-buffer-reimplementation][piece tree]]) instead of either. Rather then storing the buffer chopped up in a pieces and update those as you go instead, you store the original buffer and then each edit is stored separately as part of table or tree.  This makes things like undo and redo builtin to the data structure.

There are no optimized implementation of a piece table in rust, at least that I could find.
*** what should you use?
So is this to say that everyone should switch over at start using gap buffers? Not quite. Ropes are really powerful data structure, and they never have the latency spikes associated with resizing or moving the gap. In real world editing scenario's it isn't the best case or even the average case, it's the worst case tail latency that really matters. With ropes you get worse case O(logn) behavior for all editing operations. As files get larger the O(N) behavior of gap buffers starts to shows itself.

And ropes have other benefits besides the good performance. Both Crop and Ropey support concurrent access from multiple threads.This lets you take snapshots to do asynchronous saves, backups, or multi-user edits.

Despite all that, gap buffers showed they can do quite well when placed against more "advanced" data structures. Be very careful before you bet against arrays.


**** have a comment?
[fn:1] GB here means 2^30, as it should when [[https://www.merriam-webster.com/dictionary/kilobyte][talking about base-2 memory]]. The only people who think it should be 10^9 are hard drive salesmen and the type of people who like to correct all their friends by saying "it's centripetal, not centrifugal force!". And also "gibibyte" sounds like Pokémon invented by a first grader.

[fn:2] [[https://github.com/josephg/jumprope-rs/issues/5][github.com/josephg/jumprope-rs/issues/5]]
