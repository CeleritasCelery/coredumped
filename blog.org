#+STARTUP: content
#+AUTHOR: Troy Hinckley
#+HUGO_BASE_DIR: .

* Main
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: .
  :END:

** DONE [#B] About
:PROPERTIES:
:EXPORT_FILE_NAME: about
:END:

I am an engineer with interests in Emacs, Programming languages, performance, and compilers. Currently working in hardware design at Intel in Colorado.

* Posts
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: post
  :END:

** DONE Using org mode to write email for outlook :org_mode:emacs:
:PROPERTIES:
:EXPORT_DATE: 2019-02-08
:EXPORT_FILE_NAME: outlook-email-in-org-mode
:END:
I see many threads on Reddit and blog posts about using email inside Emacs. I mean, I already have =org-mode= which organizing my whole digital life. But then all my work email is provided through outlook, which does not allow me to fetch email with anything other then their proprietary software.

Microsoft outlooked was designed to be used by people writing marketing emails, not people talking about code. There is no way to distinguish what is code from what is text, or call our programming symbols from the rest of the prose. Emacs =org-mode= on the other hand, was built for working in a technical environment.

Thankfully org makes it easy to export any heading to HTML. The hard part is getting that HTML into outlook. Most of the ideas presented here were taken this [[http://bnbeckwith.com/blog/org-mode-to-outlook.html][post]], and then expanded on.

The process I use to write my email with =org-mode= is as follows
1. write the email in an org capture buffer
2. Use a custom function to copy the exported HTML to the clipboard
3. go to outlook and use a custom VBA function to insert the HTML from
   the clipboard as formatted text

*** Setting up Emacs
**** Using an org capture buffer
Using an org capture buffer is perfect for writing email, because I can save it as a draft if needed, or export the contents and then throw the buffer away. Also, most of the time, the content of interest is what I am working on that moment, so I have everything at hand. Here is the simple template that I use to write emails.
#+BEGIN_SRC emacs-lisp -n
  (add-to-list
   'org-capture-templates
   '("e" "Email"
     entry (expand-file-name "email.org" org-directory)
     "* %?" :empty-lines 1))
#+END_SRC

**** Exporting from org-mode
Normally if you wanted to export an org header as HTML, you would use =C-c C-e= to open the export menu. =hH= will open a dedicated buffer with the HTML contents of your org file. From there you can copy the whole buffer. However I find it much faster to use this helper function (bound to =C-x M-e=).

#+BEGIN_SRC emacs-lisp +n
  (defun export-org-email ()
    "Export the current email org buffer and copy it to the
  clipboard"
    (interactive)
    (let ((org-export-show-temporary-export-buffer nil)
          (org-html-head (org-email-html-head)))
      (org-html-export-as-html)
      (with-current-buffer "*Org HTML Export*"
        (kill-new (buffer-string)))
      (message "HTML copied to clipboard")))
#+END_SRC

**** Better CSS for export
The default HTML exported by org is spartan to say the least. Thankfully it is pretty easy to define some custom to CSS that looks prettier and plays nicer with outlooks HTML rendering engine. The outlook compatible HTML I use is located [[https://github.com/CeleritasCelery/org-html-themes/blob/master/styles/email/css/email.css][here]]. The function below adds my CSS to =org-html-head=. It is called by =export-org-email= from the previous section.

As you can see in the function below, I have this CSS at the local path =~/org/org-html-themes/styles/email/css/email.css=. You will need to change this to where ever you keep the CSS file.
#+BEGIN_SRC emacs-lisp +n
  (defun org-email-html-head ()
    "Create the header with CSS for use with email"
    (concat
     "<style type=\"text/css\">\n"
     "<!--/*--><![CDATA[/*><!--*/\n"
     (with-temp-buffer
       (insert-file-contents
        "~/org/org-html-themes/styles/email/css/email.css")
       (buffer-string))
     "/*]]>*/-->\n"
     "</style>\n"))
#+END_SRC

*** setting up outlook
**** Getting the HTML into outlook
This is the tricky part. outlook does not make it easy to insert HTML inline. I had to learn some VBA and use the outlook code editor. I hope I never have to do that again.

To add a function to outlook
1. Press =Alt-F11= to bring up the VBA editor.
2. You should see the default project. Change this project name to something more appropriate. Note that the Project name *MUST NOT* be the name of the function (~PrependClipboardHTML~) so name it something else.
3. Right click on the project to add a new module and copy in the function from below

#+BEGIN_SRC vb
Sub PrependClipboardHTML()
    Dim email As Outlook.MailItem
    Dim cBoard As DataObject

    Set email = Application.ActiveInspector.CurrentItem
    Set cBoard = New DataObject

    cBoard.GetFromClipboard
    email.HTMLBody = cBoard.GetText + email.HTMLBody

    Set cBoard = Nothing
    Set email = Nothing
End Sub
#+END_SRC

**** Fix object library
This step may not apply to everyone, but in order to get this to work, I also had to add the =Microsoft Forms 2.0 Object Library= to the References. I figured this out by looking at [[https://www.reddit.com/r/orgmode/comments/74k2mx/org_link_to_message_within_outlook_2016/][this]] Reddit thread.

1. Click on =Tools= in the menu bar (or use =Alt-t=).
2. Select =References...=
3. Select =Browse...=
4. Browse to =C:\Windows\System32\FM20.DLL= and select open

**** Add to Quick Access Toolbar
This function only makes sense in context of an email. To enable it there, add it to the quick access toolbar at the top.

1. Press =Ctrl-n= to open up a new email.
2. Select the little down arrow at the very top for the =Customize Quick Access Toolbar= menu.
3. Select =More Commands=.
4. In the drop down for =choose commands from:= select =Macros=. You should see the =PrependClipboardHTML= macro you created here.
5. Add it to the right hand side pane with the =Add >>= button.
6. Click on =Modify...= to change the icon and display name. You can also use the arrow to change the ordering in the =Quick Access Toolbar=

Now clicking on that button will copy clipboard contents into the email as HTML. Our raw HTML exported from Org mode gets inserted nicely and we gain the formatting desired.

The other bonus (or maybe the main point) is that now you can also use a built-in shorcut for the Quick Access Toolbar commands to run this one. By pressing =Alt=, you can see a number by your command.

**** Matching the default font in Outlook
It is nice sometimes to have the default font in outlook match what you are exporting from org mode. To make this happen, do the following steps in Outlook.

1. On the =File= tab, choose =Options= > =Mail=.
2. Under =Compose messages=, choose =Stationery and Fonts=.
3. On the =Personal Stationery= tab, under =New mail messages= or =Replying or forwarding messages=, choose Font.
4. In the =Font= box, choose the font, style, size, and color that you want to use. You can see a preview of your changes as you make them.
5. Choose =OK= three times to return to Outlook.

*** Bonus Content
Here are a few more ideas that are not necessary for this workflow but
are useful to me.

**** More advanced VBA
The =PrependClipboardHTML= function I showed above is not actually the version I use. But I chose to mention present it as the solution because it is simple and works well. This more advanced version has two differences

1. Works with inline email replies
2. If the subject line is empty, the HTML header at the start of the body is used as the subject line. This allows you add the subject line in org-mode and have it automatcially inserted.

#+BEGIN_SRC vb
  Sub PrependClipboardHTML()
      Dim email As Outlook.MailItem
      Dim cBoard As DataObject

      Set email = GetCurrentItem()
      Set cBoard = New DataObject

      cBoard.GetFromClipboard

      Dim sText As String
      Dim headerStart As Integer
      Dim headerStartClose As Integer
      Dim HTMLPre As String
      Dim HTMLPost As String
      Dim subject As String
      Dim paragraphStart As Integer

      Dim headerEndStr As String
      Const titleText = "<h1 class=""title"">"

      headerEndStr = "</h1>"
      headerStart = Len(titleText)

      sText = cBoard.GetText
      HTMLPre = sText

      headerStart = InStr(sText, titleText)
      If Not headerStart > 0 Then
          ' if no title text, we use the starting header
          headerStart = InStr(sText, "<h2 id=")
          headerEndStr = "</h2>"
      End If

      ' we use the first header as the subject line if the subject line is empty
      If headerStart > 0 Then
          headerStartClose = InStr(headerStart, sText, ">") + 1
          Dim headerEnd As Integer
          headerEnd = InStr(headerStartClose, sText, headerEndStr)
          If headerEnd > 0 Then
              subject = Mid(sText, _
                  headerStartClose, _
                  headerEnd - headerStartClose)
              HTMLPre = Mid(sText, 1, headerStart - 1) ' arrays start at 1...
              HTMLPost = Mid(sText, headerEnd + Len(headerEndStr))
          End If
      End If

      email.HTMLBody = HTMLPre + HTMLPost + email.HTMLBody
      ' only use the HTML subject if an email subject is absent
      If Len(email.subject) = 0 Then
          email.subject = subject
      End If

      ' deallocate objects
      Set cBoard = Nothing
      Set email = Nothing

  End Sub

  ' Get either the active email item or the current window
  Function GetCurrentItem() As Object
      Dim objApp As Outlook.Application

      Set objApp = Application

      On Error Resume Next
      Select Case TypeName(objApp.ActiveWindow)
          Case "Explorer"
              Set GetCurrentItem = objApp.ActiveExplorer.ActiveInlineResponse
          Case "Inspector"
              Set GetCurrentItem = objApp.activeInspector.CurrentItem
      End Select

      Set objApp = Nothing
  End Function
#+END_SRC
**** Normalize outlook formatting
Unless you disable it, outlook will try and "prettify" some characters as you type with non ascii-compatible versions. This means that you will often encounter errors when copying code out of outlook and trying to paste into a shell or source file. The following function takes the last paste normalizes it to be ascii compatible.

#+BEGIN_SRC emacs-lisp
  (defun normalize-text (beg end)
    "normalize characters used in Microsoft formatting"
    (let* ((orig-text (buffer-substring beg end))
           (normalized-text
            (thread-last orig-text
              (replace-regexp-in-string "–" "--")
              (replace-regexp-in-string (rx (char "‘’")) "'")
              (replace-regexp-in-string (rx (char "“”")) "\""))))
      (unless (equal orig-text normalized-text)
        (save-excursion
          (goto-char beg)
          (delete-region beg end)
          (insert normalized-text)))))

  (defun normalize-region (beg end)
    "normalize the last paste, or if region is selected, normalize
  that region."
    (interactive "r")
    (if (region-active-p)
        (progn (normalize-text beg end)
               (deactivate-mark))
      (apply #'normalize-text
             (cl-sort (list (point) (mark t)) '<))))
#+END_SRC
**** Have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/gxrg0b/writing_email_for_outlook_inside_emacs/?utm_source=share&utm_medium=web2x&context=3][Reddit]] or send me an [[mailto:troy.hinckley@dabrev.com][email]]

** DONE Native shell completion in Emacs :shell:emacs:
:PROPERTIES:
:EXPORT_DATE: 2020-01-04
:EXPORT_FILE_NAME: native-shell-completion-in-emacs
:END:
I am obsessed with autocompletion in shell mode. Running a shell in ~shell-mode~ instead of a terminal emulator has so many advantages. You can treat the whole buffer just like a normal Emacs buffer. You can copy and paste and edit the line normally. You can hook it into native Emacs functionality. You can even [[https://github.com/riscy/shx-for-emacs][display images!]]

However there is one big disadvantage. You loose access to the state the shell. This means that you have to do tricks like ~shell-dirtrack-mode~ just to make sure you are in the right directory. It also means that all the native shell completions are not available. I have tried to approach this problem from multiple angles with packages like [[https://github.com/CeleritasCelery/company-fish][this]], [[https://github.com/CeleritasCelery/company-async-files][this]], and [[https://github.com/CeleritasCelery/company-arguments][this]]. (Yes, I have written a half dozen modes to try solve this.) The most popular package to try and solve this is [[https://github.com/szermatt/emacs-bash-completion][bash-shell-completion]]. However all these solutions have the same problem that they don't actually reflect the internal state of the shell, they are just close approximations.

But the shell knows its own state. In a normal terminal emulator, tab completion will give you access to it. When looking at my shell buffer, it seems that the information I want is just below the surface. If only there was some way to access it. I am not the only one who has had this thought. There is a [[https://stackoverflow.com/questions/163591/bash-autocompletion-in-emacs-shell-mode][stackoverflow]] with over 26,000 views asking this same question. But no one has managed to access the native tab completion before. So I determined to solve this once and for all.

*** The curious case of bash
If you use csh and send some text followed by the tab character, it will print out all possible completions. But not bash. Try this code and you get nothing.
#+BEGIN_SRC lisp
(comint-simple-send (get-buffer-process (current-buffer)) "git\t\x15")
#+END_SRC

It works in the terminal but not in the Emacs shell. What conspiracy is this? Turns out that Emacs and bash have put a lot of effort into making sure completion does not work. The first thing to notice is that ~explicit-bash-args~ contains the argument ~--no-editing~, which will disable readline completion. Let get rid of that shall we?
#+BEGIN_SRC lisp
  (setq explicit-bash-args
            (delete "--noediting" explicit-bash-args))
#+END_SRC

However removing that still does not enable tab completion. There must be something else going on here. This time it is on the bash side. Looking in the source code we see the follow block.
#+BEGIN_SRC c :hl_lines 21
  term = get_string_value ("TERM");
  emacs = get_string_value ("EMACS");
  inside_emacs = get_string_value ("INSIDE_EMACS");

  if (inside_emacs)
    {
      emacs_term = strstr (inside_emacs, ",term:") != 0;
      in_emacs = 1;
    }
   else if (emacs)
     {
       /* Infer whether we are in an older Emacs. */
       emacs_term = strstr (emacs, " (term:") != 0;
       in_emacs = emacs_term || STREQ (emacs, "t");
     }
   else
     in_emacs = emacs_term = 0;

  /* Not sure any emacs terminal emulator sets TERM=emacs any more */
  no_line_editing |= STREQ (term, "emacs");
  no_line_editing |= in_emacs && STREQ (term, "dumb");
#+END_SRC

For some reason that I can't explain, bash has special code for running inside Emacs. Lo and behold, if ~$TERM~ is ~dumb~ and ~$INSIDE_EMACS~ is set, then line editing is disabled by the shell itself. Any reason for this? I would love to know. Changing ~$TERM~ to something other then ~dumb~ fixes the issue, but then programs might not interpret our terminals capabilities correct. The best thing to do is remove the environment variable ~$INSIDE_EMACS~. Doesn't seem to do anything useful after all.
#+BEGIN_SRC lisp
(advice-add 'comint-term-environment
            :filter-return (lambda (env) (cons "INSIDE_EMACS" env)))
#+END_SRC

And with that we have working tab completion in bash!

*** Making completion "Emacsy"
We could stop here and just create a function to send tab to the underlying process. It would behave exactly like the terminal does. But this is Emacs. The whole reason we are using ~shell-mode~ in the first place is because things in Emacs are nicer then things in the terminal. We have things like ~completion-at-point~ and ~company-mode~ that would make a terminal emulators head spin. Makes sense to take advantage of that. So I created a new package called [[https://github.com/CeleritasCelery/emacs-native-shell-complete][native-complete]] that talks to the underlying process and gets the actual completions that the shell would natively provide. No more trying to use other packages to /guess/ the shells state, or starting another process that may be out of sync. It even supports invoking subshells! This effort is still work in progress, and many shells have yet to be tested. As with many things, it is not as simple in the implementation as it should be.

I hope this is the shell-mode completion Endgame. I don't think I can take much more.

**** Have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/ek8v0e/native_shell_completion_in_emacs/?utm_source=share&utm_medium=web2x&context=3][Reddit]] or send me an [[mailto:troy.hinckley@dabrev.com][email]]
** DONE [#B] When pure function lie :design:emacs:
:PROPERTIES:
:EXPORT_DATE: 2021-04-07
:EXPORT_FILE_NAME: when-pure-functions-lie
:END:
Here is a simple question. Given the lisp function below (and that the function is not advised) what will the output be of ~(foo)~?
#+begin_src emacs-lisp
  (defun foo ()
    "foo")
#+end_src

Seems pretty simple right? How could the answer be anything other then ~"foo"~? It is just returning a constant string. Or is it? The real answer to this question is...

We have no idea. It can could be any string of length 3.

How can this be? Well it turns out that constant literals in lisp (common lisp, emacs lisp, etc) are not constant. They can be modified at runtime like any other variable. Here is an elisp example that does just that.
#+begin_src emacs-lisp
  (defun foo ()
    "foo")

  (foo) => "foo"

  (defun bar ()
    (let ((x (foo)))
      (aset x 0 ?福)
      x))

  (bar) => "福oo"
  (foo) => "福oo"
#+end_src

Look at that! We have modified the behavior of another function using only its return value. We might have considered ~foo~ a [[https://en.wikipedia.org/wiki/Pure_function][pure function]]. However this cannot be the case because we can return different values for the same input.

This is true whether it's dynamic or lexically bound, interpreted or byte compiled. And there is no way to get the original literal value back without redefining the function. Which means that once you try and instrument the function to see what is going on, the problem magically goes away! Here is another example where we are able to change a different constant literal in the same function. Since ~foo~ is byte-compiled, the two constants are mapped to the same mutable object. So changing one changes the other!
  #+begin_src emacs-lisp
    (defvar global)

    (byte-compile
     (defun foo (x)
       (setq global "foo")
       (concat "foo" x)))

    (foo "bar") => "foobar"

    (aset global 2 ?x)

    (foo "bar") => "foxbar"
#+end_src

*** Why does this happen?
Strings in lisp are mutable, and when you pass a string to something it actually passes a reference to it. But once you have a reference to a string, you can modify it however you want. Which means that you are modifying the original string that was part of the constant list of the function!

As far as I could find, this behavior is unique to lisp (and it works with literal lists and vectors as well). Other dynamic languages don't have this property. For example, the python code below does not change the original list[fn:1].
#+begin_src python
  def foo():
      return [1, 2, 3]

  def bar():
      x = foo()
      x[0] = 0
      return x

  print(foo()) => [1, 2, 3]
  print(bar()) => [0, 2, 3]
  print(foo()) => [1, 2, 3]
#+end_src

But the elisp version will
#+begin_src emacs-lisp
  (defun foo ()
    '[1 2 3])

  (defun bar ()
    (let ((x (foo)))
      (aset x 0 0)
      x))

  (foo) => [1 2 3]
  (bar) => [0 2 3]
  (foo) => [0 2 3]
#+end_src

I would venture to say that anytime you overwrite a constant literal it almost certainly a bug. And a very hard to debug one at that! So why does lisp allow this? I imagine a large part of it is simplifying the implementation. You don't have to check if the value you are modifying is constant or not, you just mutate it. Another part of it is that Common Lisp was conceived in a time when string where just vectors of ascii characters (Similar to C). That made modifying them like modifying normal arrays. But now with the advent of unicode, changing a "character" of string is not so easy. It can involve shifting the entire string or even reallocating depending on the size of the code point. This is why most modern languages have immutable strings by default.

This is another reason that it is hard to make a true multi-threaded elisp. You can't share function between threads when "normal" code can change the behavior of functions being used in different threads. Functions that look pure might be changing under the hood. In this situation you are cross-pollinating your code with mutable data. I should add that this really has nothing to do with [[https://en.wikipedia.org/wiki/Homoiconicity][homoiconicity]]. You could still have a fully homoiconic language without the ability to overwrite constant literals.

When I first saw this behavior, I thought for sure that I found a bug in the language implementation. This couldn't possibly be intentional. But after investigating more, I found that this was expected... at least if you are lisp programmer.
****  Have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/mm70re/when_pure_functions_lie/?utm_source=share&utm_medium=web2x&context=3][Reddit]] or send me an [[mailto:troy.hinckley@dabrev.com][email]]

[fn:1] As several people [[https://www.reddit.com/r/emacs/comments/mm70re/when_pure_functions_lie/gtq1oir?utm_source=share&utm_medium=web2x&context=3][pointed out]], the reason this works in python is because it is making a copy of the list every time it returns. You could introduce the same issue in python using default arguments, which are not copied. To protect against this in lisp, you can use ~(vector 1 2 3)~ instead of ~'(1 2 3)~ and it will make a copy of the vector.

** DONE [#B] Building an Emacs lisp VM in Rust :emacs:rust:
:PROPERTIES:
:EXPORT_DATE: 2021-10-21
:EXPORT_FILE_NAME: building-an-emacs-lisp-vm-in-rust
:END:

About a year ago I was bitten by the PL bug. It started with reading [[http://craftinginterpreters.com/][Crafting Interpreters]] and discovering the wonders hidden under the hood of a compiler. I am also been a big fan of Emacs, and this started to get me interested in how it's interpreter works. At the same time, I was reading the Rust book and trying to understand the concepts there. This all came to a head, and I decided to write an Emacs Lisp interpreter called [[https://github.com/CeleritasCelery/rune][rune]] in Rust.

My goal for this project is to bootstrap [[https://github.com/emacs-mirror/emacs/blob/master/lisp/emacs-lisp/bytecomp.el][bytecomp.el]] and use the Emacs compiler with my bytecode vm using only stable Rust. I have not reached that goal yet, but I have bootstrapped several core Emacs lisp files. At this point I have a enough of an interpreter that I want to share an update and mention some of the trade-offs and other things I have learned.
*** Overview
**** Tree walk or bytecode?
Emacs has 3 seperate execution engines: a tree walk interpreter, a Bytecode VM, and ([[https://git.savannah.gnu.org/gitweb/?p=emacs.git;a=commit;h=289000eee729689b0cf362a21baa40ac7f9506f6][recently!]]) native compile. They all provide their own sets of trade-offs, but that also mean that any new feature needs to be implemented up to 3 times. I didn't want the duplicate work, so I opted to only have a byte code VM and no interpreter. This turned out to be harder than I initially thought. All the early elisp files assume that you are using an interpreter. Macros are often used before they are defined because the interpreter has lazy-macro expansion. This is harder for a byte-compiler because you want to expand the macro's at compile time instead of run time. I ended up needing to make some tweaks to the ordering and structure of the [[https://github.com/CeleritasCelery/rune/tree/master/lisp][lisp files]] to support a bytecode-only bootstrap.
**** Object representation
A critical part of any dynamic language is how to represent types at runtime. Since you will frequently be boxing and unboxing values, you want these to be both time and space efficient.

Rust provides a strong candidate in its enums, but you are limited to the representations that they provide. Most of the time this isn't a problem. However, because of the language specification that pointers are a full word, you can’t normally use optimizations like [[https://piotrduperas.com/posts/nan-boxing][NaN-boxing]] or [[https://en.wikipedia.org/wiki/Tagged_pointer][pointer tagging]] in Rust. Therefore, when I was initially defining object type as a ~union~.
#+begin_src rust
  union Object<'ob> {
      tag: Tag,
      data: i64,
      marker: PhantomData<&'ob ()>,
  }
#+end_src

Then in the boxing and unboxing code, I could check the type of the tag field and reinterpret the bits as whatever type was needed. This had the advantage of being extremely flexible (since I had complete control over the bit layout and representation) but it also had some drawbacks compared to a proper Rust enum.

1. Unboxing requires ~unsafe~ code
2. Must manually match the tag to the right data type. There are no compiler checks here.
3. No way to ~match~ directly on the union. Need to create an accessor functions to get the underlying value as an enum.
4. Can't use variants as values. With an Enum you can use ~Option::None~ or ~Option::Some(T)~ as values, but instead you have to create constants to represent common values.
5. No debugger support. A union is completely opaque to the Rust debugger.
6. No destructuring support. Since Rust's pattern syntax does not understand my type, there is no way to do the following
#+begin_src rust
  match object {
      Some(Object::Nil | Object::Int(_)) => ...,
      Some(Object::String(s)) => ...,
      _ => ...
  }
#+end_src

Given all this, I decided to try and see if I could make an enum that still had the space optimization of a tagged pointer (word width). This is done with a ~Data~ type, which is 56 bits wide.

#+begin_src rust
#[derive(Copy, Clone)]
pub struct Data<T> {
    data: [u8; 7],
    marker: PhantomData<T>,
}
#+end_src

When this is used in an enum the result is a 64 bit object (The same size as a tagged pointer on 64-bit systems). [fn:5]

#+begin_src rust
pub enum Object<'ob> {
    Int(Fixnum),
    Float(Data<&'ob f64>),
    Symbol(Data<Symbol>),
    True,
    Nil,
    Cons(Data<&'ob Cons<'ob>>),
    Vec(Data<&'ob Vec<Object<'ob>>>),
    String(Data<&'ob String>),
    LispFn(Data<&'ob LispFn<'ob>>),
    SubrFn(Data<&'ob SubrFn>),
}
#+end_src

These ~Data~ structures have a ~inner~ method that transforms the 56 bit payload into a full width pointer. This is done by shifting the pointer so that the bottom byte can be used for the discriminant.

#+begin_src rust
  impl<T> Data<T> {
      const fn expand(self) -> i64 {
          let data = self.data;
          let whole = [
              0, data[0], data[1], data[2], data[3], data[4], data[5], data[6],
          ];
          i64::from_le_bytes(whole) >> 8
      }
  }

  impl<'a, T> Inner<&'a T> for Data<&'a T> {
      fn inner(self) -> &'a T {
          let bits = self.expand();
          let ptr = bits as *const T;
          unsafe { &*ptr }
      }
  }
#+end_src

Rust optimizes this into a simple 8 bit shift for boxing and unboxing. The Data type can also hold additional information like mutability.

Overall I am very happy with this approach. But is has limitations of it's own.

One thing that would make this approach much simpler is if we could use the ~Deref~ trait to unbox ~Data<T>~. But this only works some of the time. Since deref takes ~&self~, it ties the lifetime of the borrow of ~self~ to the lifetime of the unboxed data.

#+begin_src rust
  impl<'a, T> Deref for Data<&'a T> {
      type Target = T;
      fn deref(&'_ self) -> &'a Self::Target {
          self.inner()
      }
  }
#+end_src

This makes it much less useful. I wish Rust had a trait like ~DerefCopy~ that transformed ~self~ to ~T~ instead of transforming ~&self~ to ~&T~ and still offered [[https://doc.rust-lang.org/std/ops/trait.Deref.html#more-on-deref-coercion][deref coercion]].

#+begin_src rust
  impl<'a, T> DerefCopy for Data<&'a T> {
      type Target = &'a T;
      fn deref(self) -> &Self::Target {
          self.inner()
      }
  }
#+end_src

Currently I have repurposed the ~Not~ operator as my own deref function. It doesn’t offer any of the magic of deref coercion, but it is more concise then calling ~inner()~ every time you need to unbox something (which is very frequent). And since it takes ~self~ and not ~&self~, it doesn't run into the lifetime issues deref does. So when I need to unbox something I can use ~!data~ to achieve this.

#+begin_src rust
impl<T> Not for Data<T> where Data<T>: Inner<T> {
    type Output = T;
    fn not(self) -> Self::Output {
        self.inner()
    }
}
#+end_src

Another problem with enums is that they leave the unused portion of the allocation uninitialized  [fn:6]. This can allow optimizations when the kind is known ahead of time and is smaller than the full enum size. However this also prevents other optimizations from being used.

For example, Emacs provides several different version of the ~equal~ function based on how strict they are. The simplest is ~eq~, which checks if two variables point to the same object. This specializes to a single word size comparison (1 instruction), because two objects that point to the same object will have the same bit representation. Unfortunately this optimization is not possible with Rust enums, because part of the enum can be uninitialized, and it is UB to read uninitialized memory. Therefore implementing ~eq~ is much more expensive with a Rust enum then it is with a union. This one of the disadvantages of not have control over the layout of your type.

To work around this, I have tried implementing the fieldless variants of my enum as having a field of type ~Data<()>~ which is always zero. This is slightly less ergonomic, but give back the power to compare objects directly by their bit representation.

**** Defining functions
I can’t take credit for this, as the idea came from [[https://github.com/remacs/remacs/tree/master/rust_src/remacs-macros][remacs]] (the original Emacs in Rust project), but it really showcases the power of Rust procedural macros. The ~defun~ macro is applied to any normal Rust function and it then becomes callable from lisp.

#+begin_src rust
  #[defun(name = "-")]
  pub(crate) fn sub(number: Option<Number>, numbers: &[Number]) -> NumberValue {
      match number {
          Some(num) => {
              let num = num.val();
              if numbers.is_empty() {
                  -num
              } else {
                  numbers.iter().fold(num, |acc, x| acc - x.val())
              }
          }
          None => Int(0),
      }
  }
#+end_src

This macro creates a wrapper around the function that transforms lisp objects into Rust types and back, handling any type errors along the way. The type signature of the Rust function also gets converted to the type signature in lisp; ~Option~ types become ~&optional~ and slices become ~&rest~. For example the function signature above will become ~(- &optional NUMBER &rest NUMBERS)~.This makes it easy to use the function in Rust or lisp, and the syntax is much cleaner then the [[https://github.com/emacs-mirror/emacs/blob/3cabf64131b93e1b0510a01bcce8efde38b20bc4/src/lisp.h#L3050][DEFUN]] macro used in the the Emacs C code.

*** Interesting learnings along the way
**** Generics in Rust
Generics are a really powerful feature that let you build reusable data structures and help eliminate some boilerplate. Less code means less bugs. Generics are particular useful in conjunction with traits, letting you implement them for a range of types. However, I found that in practice generics were less useful than they could have been due to the lack of [[https://rust-lang.github.io/rfcs/1210-impl-specialization.html][specialization]]. This absence means that anytime you need to specialize for one type you completely lose the use of generics for that function/trait [fn:7]. Because of this I ended up implementing many of the traits with macros instead of generics. If specialization is ever stabilized, it will remove hundreds of lines of boilerplate from the code base. But it looks like that is still a [[https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/][ways off]].

**** Garbage collection
I have not currently implemented a garbage collector for my interpreter, though it doesn't leak memory. This works because all objects are “owned” by an arena, and all the lifetimes of objects are tied to the borrow of the that arena. So when the arena goes out of scope, so do all the objects it owns. This works fine for bootstrapping, but there is no freeing of unused memory. I have done a lot of reading on garbage collectors and they are considered very tricky to get right. As Bob Nystrom [[http://craftinginterpreters.com/garbage-collection.html#garbage-collection-bugs][said]], “garbage collector bugs really are some of the grossest invertebrates out there.”

Rust has some unique offerings that promise to not only make garbage collectors easier to implement, but safer to use as well. I am not going to go into detail here because you can find a great overview of different approaches in this [[https://manishearth.github.io/blog/2021/04/05/a-tour-of-safe-tracing-gc-designs-in-rust/][series of blog posts]].

The most interesting crates to me are ones that use Rust's borrow checker to ensure that it is safe to run the collector. All objects have a lifetime tied to a ~Context~ object. Anytime the Garbage collector runs it takes a ~&mut self~, ensuring all objects it created can't be accessed afterwards. In order to keep objects alive you need to root them. This is either done with stack or linked list. Some examples of this approach are [[https://github.com/asajeffrey/josephine][joesphine]] and [[https://github.com/withoutboats/shifgrethor][shifgrethor]].

Another similar approach is the concept of [[https://raw.githubusercontent.com/Gankro/thesis/master/thesis.pdf][generativity]], which is essentially using a unique lifetime to brand objects so they cannot be unified with other arenas. The [[https://crates.io/crates/gc-arena][gc-arena]] and [[https://crates.io/crates/cell-gc][cell-gc ]]are example of this. One thing is for sure, these libraries will become much easier to use if Rust ever gets the ability to track stack roots[fn:1]. Until that time there is still an wide space to be explored.

The last thing that makes garbage collectors difficult in Rust is the that the [[https://github.com/rust-lang/wg-allocators][allocator API]] is still unstable, and probably won't be stabilized anytime soon. Some gc algorithm's rely on particular layouts of data to work correctly. Currently you need to either use the changing API in nightly or implement things yourself with pointers.

**** Fixing lifetime issues
When I first created lisp objects, they were unions with raw boxed pointers. After all, this is what you would have in C. However, after facing several memory errors, I decided to take advantage of Rust lifetime system and add lifetimes to all objects. They now hold a ~PhantomData~ of a reference. When I first made this change it lead to a lot of pain. I learned it is very valuable to really step back and actually think about borrow checker messages. Oftentimes rather then fighting the borrow checker, you are better off restructuring your code to make it more lifetime friendly. Once I did a major refactor where data flows from main to the rest of the program most of my lifetimes issues just disappeared. Another thing to keep in mind is that just because Rust /allows/ your lifetimes doesn't make them /correct/. All that rustc cares about is that your lifetimes are not memory unsafe; it doesn’t care if they are [[https://github.com/pretzelhammer/rust-blog/blob/master/posts/common-rust-lifetime-misconceptions.md/#5-if-it-compiles-then-my-lifetime-annotations-are-correct][correct]]. It is up to you as the developer to make sure your lifetimes are right. Most often what I needed to do to correct my lifetimes was to split them up. Forcing Rust to unify unrelated lifetimes is guaranteed to cause more pain then needed.

**** Globals vs multi-threaded
I was initially inspired to do this project by /Crafting Interpreters/ and reading the Emacs internals. Both of these programs make heavy use of globals to store and manipulate state, which is very common in C. However Rust takes a different stand. In Rust there is no such thing as single threaded code. Even code that does not rely on any concurrency constructs is expected to work without issues in a multi-threaded environment. This means all globals must be wrapped in a concurrency safe type.

However, I was still convinced that I wanted to do things the "C" way. It made following my templates (Lox and Emacs) much easier. Accessing a raw global is cheap; Accessing it though a mutex is not. I "knew" that my interpreter was not multi-threaded and I did not want to pay that overhead. However, finding out how to implement raw globals was no easy task. It took some digging, but I did discover that you can implement C-style zero-cost globals in Rust with some unsafe code. Not too long after I implemented that I began to run into random test failures. I found much to my surprise that even the test runner in Rust is multi-threaded! At this point, I broke down and decided to get on board with the Rust approach to concurrency. I moved all globals to the stack or put them behind a mutex. It wasn’t as bad as I feared.

***** The seeds of parallelism
As part of the move to a concurrency safe runtime, I started thinking about what it would take to have a true multi-threaded Emacs lisp. To experiment with this, I set it up so that all functions are shared between threads with atomics; But values are thread local. This brought up some interesting challenges that Emacs lisp presents concurrency, all related to mutability and aliasing.

For one, [[https://coredumped.dev/2021/04/07/when-pure-function-lie/][function literals are mutable in lisp]]. This means you can change a function by mutating it’s return value. If functions are shared between threads, then they can't be mutable; Otherwise you expose yourself to dataraces. In Common Lisp they just say "yolo!" and make mutating a function literal undefined behavior. However, you can’t easily tell when you are doing this; It can often be very far from the call site.

Another issue is that aliasing is very common in elisp. This generally isn’t an issue in single-threaded code, but becomes a source of very difficult bugs in a multi-threaded world. You need to either make all objects concurrency safe (which is very expensive) or prevent threads from mutably aliasing each other’s data. This is one of the areas where Rust really shines, but would require a lot of hard trade-offs in lisp.

For example, concurrency in Emacs would not be very useful without the ability to share buffers. If you share buffers, you also need a way to share buffer local variables; and buffer local variables can share data (cons cells, strings, and vectors) with other local variables. There is no way to share a buffer with another thread without also sharing your entire environment. At some point I plan to write more about potential multi-threading in Emacs, but that will have to be saved for a future post.

*** Rust as a language backend
Overall, I have come to love Rust! It makes systems programming feel accessible. And the community is absolutely awesome [fn:2]. I've never had a question that I was not able to get help with. That being said, implementing an interpreter for a dynamic language in Rust is particularly challenging because the host language does not[fn:3] follow Rust's rules around mutability and aliasing. To solve this you need to either do runtime accounting using ~Rc<RefCell<T>>~ (which is expensive and leaks cycles), or deal with upholding all of Rust invariants manually in unsafe code. Neither is a very attractive proposition.

Speaking of unsafe, you often hear that writing unsafe code is "just like writing C". That is not really true. Rust has more invariants that need to be upheld then does C, especially related to mutability, aliasing, traits, layout, initialization, and dropping. All these invariants need to be considered when writing unsafe code and can lead to [[https://www.youtube.com/playlist?list=PLqbS7AVVErFj1t4kWrS5vfvmHpJ0bIPio][very tricky unsound behavior]]. Many of these are either not a concern, or much less of a concern, in correct C code.

Rust also lacks a feature of C that is used to implement fast interpreter loops; [[https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables][computed goto]]. This feature can be used to implement [[https://en.wikipedia.org/wiki/Threaded_code#Direct_threading][direct threading]] without the need for assembly code, giving a sizable [[http://www.cs.toronto.edu/~matz/dissertation/matzDissertation-latex2html/node6.html][performance]] increase on some processors[fn:4]. Rust may support this in the future, but given the complex interactions this would have with the borrow checker, I doubt it. I could see future where fast Rust interpreters write their inner dispatch loop in C just to take advantage of this feature.

Now, none of this is to say that Rust is poor language for writing a dynamic language backend. On the contrary, it offers some features like sum types, unnullable pointers, and safety from concurrent data races that are really powerful.  However, some of Rust's core strengths in aliasing and mutability apply less well to this domain then they do to others.

*** Conclusion
I really gained an appreciation for the depth of the Emacs internals. That code has been around for a long time and is very mature; but at the same time, it is also under active development. Trying to implement Emacs from scratch would mean not only matching the current well-tested functionality, but also trying to keep up with the constantly changing internals. While Emacs may not be the most elegant C code base, it is certainly robust.

As for how long I plan to continue this project, I don't really know. At very least I am going to bootstrap the Emacs lisp compiler to test it against my runtime and implement a garbage collector. My expectation is either that I will learn enough about text-editors and interpreters to be able to contribute to Emacs proper, or I will find a problem in the Rust ecosystem that does not have a good solution and focus on that instead. Or I may continue to see how far I can push this project. Either way, contributions and testing are welcome. Please take a look at the [[https://github.com/CeleritasCelery/rune][code]] and give feedback. I am particularly interested in anything that could be unsound or lead to undefined behavior. This has been a great experience and I am learning more than I could have hoped.

**** Have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/qcus3f/building_an_emacs_lisp_virtual_machine_in_rust/?utm_source=share&utm_medium=web2x&context=3][Reddit]], [[https://news.ycombinator.com/item?id=29038140][Hacker News]], or send me an [[mailto:troy.hinckley@dabrev.com][email]]

[fn:1] LLVM has support for this, but is has not been moved into Rust yet.

[fn:2] That is, so long as you don't use a trigger phrase like "unsafe code" or "this works fine in C".

[fn:3] Some functional languages do have invariants around immutability, but they often use mutability under the hood.

[fn:4] I saw some claims that using threaded dispatch in CPython brought a 10-20% improvement, but I didn't see benchmarks.

[fn:5] As an added bonus converting between objects can be a no-op with the [[https://github.com/rust-lang/rust/issues/60553][arbitrary_enum_discriminant]] feature that was set to make it into 1.56. Unfortunately this was [[https://github.com/rust-lang/rust/pull/89884][recently reverted]].

[fn:6] For an example of how subtle UB can happen with enums see [[https://github.com/crossbeam-rs/crossbeam/issues/748][this crossbeam issue]].

[fn:7] This [[https://github.com/rust-lang/rust/issues/50133][issue]] shows how a seemingly innocent blanket implementation in the core can break a bunch of generics for all users due to no specialization.

** TODO [#B] compilation mode on steroids
=compile.el= is a built-in package for Emacs that Let's you run
compilations in special dedicated mode. For example, if we wanted to
run make on a project, we would call ~M-x compile~ and it would
display a command prompt which conveniently defaults to "make -k". We
could use to run other commands as well, such as ~g++~, ~go~, or
~javac~. Basically anything that you can run from the command line,
you can run in compile mode.

Once you launch the command, compile opens a new popup window that
shows the commands progress. It actively scans for errors using a
builtin list and reports the total error count in the mode line. At
any time you can navigate to an error and use ="RET"= to jump to the
file. It works great with almost no configuration for the majority of
cases.

My builds, however, are more complicated. I use a proprietary set of
build tools that might run for over 10 hours. None of the standard
error parsing regex applied to me. Since my builds take so long, I
would often have up to a dozen builds running at once. There are also
several different types of builds that need to run particular order,
assuming the previous stage passed. On top of that, some of the builds
would stall and wait for user input forever. The compilation system as
it currently stands required too much micro management and was not
powerful enough to support my workflow. So for a long time, I managed
all my builds from command line, But this was also a very slow
workflow. So I decided to investigate what compile.el could do for me.
Eventually I was able to create a workflow with smart command
dispatching, command queuing, and a compilation status popup.

Before I get started let me just say that since all my build commands
are proprietary, There is little value in showing the actual arguments
and commands used. I think that would just create additional noise
anyway, so I am replacing all of that with sudo code. It should make
it easier to adapt this workflow to your needs.

*** dispatching commands
     My first issue was that I was frequently changing the commands
     line arguments. This would require me to

*** error parsing

*** detecting stalls

*** the command queue

*** alerts when a command finishes

*** the status window

*** log file mode

** TODO [#B] Emacs in 10 years
Emacs is the editor of a lifetime. It has been around more then 40 years, and I hope it will be around another 40. There are so many editors that come in with new tech and fancy features, but within a decade or two. But I hopefully will be able to use Emacs for the rest of my life. The editor is always adapting and changing. Here I put down some of my thoughts on what I hope the future of Emacs holds, and also what I hope it does not.
*** GUI first, Terminal second
Like many old applications, Emacs started in the terminal and that is where it takes it roots. This was the only option before the advent of GUI's. And even after graphical interfaces became popular there were still so many advantages to running in a simple terminal environment. But times have changed and there are now many advantges to using Emacs as GUI.  There is a popular [[https://blog.aaronbieber.com/2016/12/29/don-t-use-terminal-emacs.html][article]] by Aaron Bieber that explains some of the reasons, so I won't reiterate those here. But I hope the future of Emacs is focused on GUI first, Terimal second. You can already see this happening with things like Harbuff and better ~pixel-scroll-mode~ support being added to Emacs 27.
**** scrolling and cursor stays put
Scrolling in Emacs has always felt awkward compared to what you would get in a normal gui application. For one thing, you cannot scroll a window without also moving the cursor. And there is no easy way to get the cursor back unless you remembered to set a mark first. This need not be.

**** draw a window anywhere
If you want to do a fully stylized popup you need to either use an overlay or a child frame. Overlays have the limitation that they cannot cross window boundaries and only work with monospaced fonts. Using frames means you popup is dependent on your window manager. For me this means that every time I show or hide a child frame it flashes white before it redraws, leading to a very jarring experience. Emacs should be able to draw a floating window anywhere in the frame that can be used to tool-tips, completion, or anything else.
**** give me back my keys!
For historical reasons, terminals treat many different keybindings as the same. For example in a terminal emulator =C-j= and =C-m= are  =<return>=, =C-i= is =<tab>=, =C-[= is ~<esc>~, etc. Emacs carries these same terminal mappings into the GUI, even though the GUI can distinguish these keys. Fixing this requires some [[https://emacs.stackexchange.com/questions/220/how-to-bind-c-i-as-different-from-tab/221#221][voodoo]] with ~input-decode-map~. The GUI version of emacs should distinguish these keys, but for compatibility they could be bound to the same function by default. But that makes it easy for users to rebind them if needed.
*** Better buffer model
**** Long lines
A common pain point with Emacs is handling files with long lines. There is a [[https://www.emacswiki.org/emacs/SoLong][new package]] in Emacs 27 that will check for long lines in a file and disable features if they are over a certain length. But this still does not get to the heart of the problem. There is a whole [[https://github.com/codygman/figure-out-emacs-long-lines-issue/blob/master/figuring-out-emacs-display-issues.org][github repo]] dedicated to trying to solve this issue. However it looks like it would need a complete rework of the Emacs buffer model.
**** Code folding
If you try and fold more then a few thousand lines Emacs will slow to a crawl. Vim and sublime can handle this fine, and many users of those editors will enable folding by default. But in Emacs that is a time bomb waiting to happen. This seems related to the work Emacs has to do every time you move the cursor. I have never been able to find a mode (including ~set-selective-display~) That is performant on large files. And large files it where you need folding the most.
*** modern runtime
**** jit compiler
There have been several attempts to create a JIT compiler for Emacs with the most [[https://lists.gnu.org/archive/html/emacs-devel/2018-08/msg00393.html][recent]] one being back in 2018. JIT stands for "Just in Time" compiler that will interprent new code straight byte-code. Currently byte code is only created if you explicityly byte-compile a function or file. This is a feature of many modern dynamic languages and could greatly improve the performance of evaluated elips code.
**** byte code optimization
Steve Yegge has a very interesting talk entitled [[http://steve-yegge.blogspot.com/2008/05/dynamic-languages-strike-back.html][Dynamic Languages Strike Back]] where he outlines some of the interesting advantages dynamic languages have over static ones. This advantage comes down to dynamic languages having a runtime process that optimize things on the fly, whereas a static language has to know everything at compile time. For example There is the concept of Trace Trees, where the runtime examines "hotspots" in the code and applies heavy optmiization to the most executed branches of that. This includes things like inlining function calls, tail call optimization, and caching values. There are a lot of really cool things that can happen with runtime environement like emacs.
Trace trees, tail call, string interning, inlining
fib(40) in python : 34
fib(40) in emacs : 131
fib(40) in c: 0.6 seconds
almost 4x worse performace
**** not stop-the-world GC
GC is a huge part of the total cost

53 seconds of GC.
13261 collections
40% of it is GC
**** make sure not to loose introspection

**** reader macros

http://xkcd.com/1638/

="\\`\\\\\\(\\(a\\|b\\|c\\)\\(d\\|e\\)\\\\)\\'"=
=r"\`\\((a|b|c)(d|e)\)\'"=

="\\(\\`\\|[^\\]\\)\\(\\\\\\\\\\)*\\(\\\\\\?\\)\\'"=
=r"(\`|[^\])(\\\\\)*(\\\?)\'"=

**** move c code to elisp
Example of line numbers

*** What I hope the future does not hold

**** concurency
Leads to sloppy code
there is a lot of headroom on in the single threaded space.
Maybe limit it to font locking and buffer refresh

**** first class extension languages
Vim and neovim can do this.

Those languages will fade. Emacs is the editor of a life time, it needs to stick around. And be comptaible long after those languages are gone.

no brow

****  browser in emacs
*** conclusion
Make sure that you praise it. Excited.

=C-c C-f=
** TODO [#B] Thoughts on Garbage collection in Emacs
What could we do with a generational garbage collector?
Be cool to create some graphs
** DONE Taking org-roam everywhere with logseq
:PROPERTIES:
:EXPORT_DATE: 2021-05-26
:EXPORT_FILE_NAME: taking-org-roam-everywhere-with-logseq
:END:
Update: 2022-05-13

I love [[https://www.orgroam.com/][org-roam]]. It lets me take notes in a way that matches how I think. It makes it easy to recall what I have learned and find connections between ideas. But there has always been one big problem with org-roam: it ties me to the desktop. When I am on the go and all I have is my phone, I don't have access to my notes.

There are some stop gap solutions to try and fix this. [[https://beorgapp.com/][Beorg]] is iOS app that supports editing org files on mobile. However, it is more focused on task management than note taking. It does not offer text search, has no way to insert "org-roam-links", and does not supported nested directories of org files. [[https://organice.200ok.ch/][Organice]] is another org mobile solution that can be used as PWA. However, it suffers from the same limitations as beorg, and has a poor editing experience (when you switch away from the app it will close your text box, making it hard to take notes on something you are reading). In the end, I was not satisfied with anything that I found: nothing allowed the same workflow I had on desktop.

Then I happened upon logseq. It is open-source, privacy-centric note taking app inspired by org-mode and [[https://roamresearch.com/][roam-research]]. It has native support for the org mode format. Since both org-roam and logseq are based around roam itself, I can use the same workflow on both apps. It has all the features you would expect of a roam replica, including full text search, page links, inline images/video's, etc. As a bonus, it is fairly easy to make it work with org-roam.
*** Configuring logseq
Logseq has an app for [[https://apps.apple.com/us/app/logseq/id1601013908?platform=ipad][iOS]] and [[https://github.com/logseq/logseq/releases/tag/nightly][Android]] (beta). These will use some shared or local storage on your device to access the notes. For me, I store my notes on iCloud Drive, which is available on both my Mac and my iPhone.

Once the org-roam files have been moved to the correct place, we need to setup logseq. First open the settings:

file:static/images/logseq-settings.png
We need set Preferred file format to ~Org~ and Preferred workflow to ~TODO/DOING~.
[[file:static/images/logseq-setting-editor.png]]

From there we need update some of the advanced settings to work with org-roam.

[[file:static/images/logseq-config-edit.png]]

We are going to add 1 line here.
~:org-mode/insert-file-link? true~

file:static/images/logseq-config-org-file-links.png

*** Configuring org-roam
For the emacs-side config we need to make sure that org-roam follows the same directory structure as logseq. The important part of ~org-roam-capture-templates~ is that new files are created in ~pages/~ and that ~org-roam-dailies-directory~ is ~journals/~. The rest can be customized as you like. This can be done with the config below.
#+begin_src emacs-lisp
  (use-package org-roam
    :custom
    (org-roam-directory "<path to logseq root>")
    (org-roam-dailies-directory "journals/")
    (org-roam-capture-templates
     '(("d" "default" plain
        "%?" :target
        (file+head "pages/${slug}.org" "#+title: ${title}\n")
        :unnarrowed t))))
#+end_src

One other issue is that when logseq creates a link, it will do so using a file link. But when org-roam creates a link it will do using a id link. Org-roam doesn't see file links as backlinks and logseq [[https://github.com/logseq/logseq/issues/3281#issuecomment-1059862531][doesn't see id links as backlinks]]. It's kind of a [[https://seuss.fandom.com/wiki/Sleeping_Moose][moose juice]] and goose juice situation. To fix this I regularly run ~org-roam-migrate-wizard~, which will convert file links to id links (among other things).
*** Taking org-roam on the go
With org-roam and logseq setup, I can now access my notes anywhere. When I am around my computer I have Emacs open and take notes in org-roam. When I am out on the go, I have logseq. I finally have access to my notes everywhere and don't have to be tied to my PC. They are a perfect match.
*** Screenshot
[[file:static/images/logseq-screen-shot.png]]

** DONE [#B] Implementing a safe garbage collector in Rust :rust:
:PROPERTIES:
:EXPORT_DATE: 2022-04-11
:EXPORT_FILE_NAME: implementing-a-safe-garbage-collector-in-rust
:END:
In my [[https://coredumped.dev/2021/10/21/building-an-emacs-lisp-vm-in-rust/][last post]] I introduced an Emacs Lisp VM I was [[https://github.com/CeleritasCelery/rune][writing in Rust]]. My stated goal at the time was to complete a garbage collector. I think Rust has some really interesting properties that will make building garbage collectors easier and safer. Many of the techniques used in my GC are not original and have been developed by other Rustaceans in previous projects.
*** Why use garbage collection?
virtually all non-trivial programs need some way to reuse memory. Rust does this by tracking every allocation statically to determine when it's no longer in use. However, this system is not flexible enough for some applications. In these cases Rust gives you [[https://doc.rust-lang.org/std/rc/struct.Rc.html][Rc]], the reference counting cell. This cell tracks the number of owners of a piece of memory at runtime. Reference counting has the advantage that is relatively easy to implement and integrates seamlessly with non-rc code. However, it also has two big downsides: It's slower[fn:5] and it can't detect cyclic data (which lisp is full of). For these reasons (and others) dynamic languages often use garbage collection (GC) to manage data.

*** Why is GC hard?
In his book /crafting interpreters/ Robert Nystrom has a whole [[http://craftinginterpreters.com/garbage-collection.html#garbage-collection-bugs][section]] dedicated to some of the "nasty bugs" you can have in a garbage collector. The problem lies in identifying all objects that are accessible in the heap. Once you have an object you know is live, it's fairly easy to trace through anything it points to and find other live data. But how do you find the pointers that don't have anything pointing to them? These pointers are problematically scattered across the stack or stored in machine registers. If you miss even one you open yourself up to memory unsafety.
**** How does Emacs solve this problem?
Emacs (and many C based GC implementations) solves this by recognizing that the stack is just a block of memory[fn:6]. If an object can't be reach from the stack, it can't be reached at all. So when garbage collection is triggered, they will dump all registers to the stack and scan the it for anything that "looks like" a pointer. I say looks like because we can't /actually/ know if something is pointer or a number in range of a pointer. There is no type information in the hardware. So anything that might be a pointer is treated as a pointer and traced. However because we aren't sure, we can't move any of the gc data. In my implementation we are building a "precise" collector that knows exactly what's a pointer and what's not. That rules out blind stack scanning.
**** Let's start from the beginning
When we allocate a new object, we know that it is unaliased (nothing has a pointer to it). But as soon as we give that pointer to user code, it becomes exposed. Problem is, we need to know when it safe to call ~drop~ and release the memory.  In C, it is up to the user to call ~free~ when they are done with it. But Rust tracks the liveness of the data with the type system. The Rust rule is this: There can be many immutable references to an object or one mutable reference (but not both). If you have immutable references, they get invalidated as soon as a mutable reference is used.

**** Affine types
This key property of Rust (called affine types) is what is used in the gc library [[https://docs.rs/josephine/latest/josephine/][Jospehine]]. They use Rust's borrow checker to ensure no references are live after collection. We do the same. All pointers into the GC heap borrowed from our allocator (called ~Arena~) via a immutable reference. When we call ~garbage_collect~, we take a ~&mut Arena~, ensuring that all heap references are no longer accessible.
#+begin_src rust
  let arena: &'ob Arena = ...;
  let object: Object<'ob> = arena.add("foo");
  use_object(object);
  arena.garbage_collect(); // Object is no longer accessible
#+end_src

However, If we  invalidate all references to the GC heap when we call ~garbage_collect~, we can't access our data at all afterwards. We obviously need something more here.
*** Rooting
What we really want is to have some pointers /preserved/ across calls to ~garbage_collect~. But we need to make sure the gc knows about these special pointers, or it will free the data they point to. We call these special pointers roots.

We have a similar problem with normal data structures. We need to get a reference to a value after we mutate something. How do we solve this problem in that case? Take the example below:
#+begin_src rust
  let mut map = HashMap::new();
  let key = "my key";
  map.insert(key, 13); // insert at key
  let value1 = map.get(key).unwrap(); // get a reference to our item
  let _ = &mut map; // take a mutable reference, invalidating our value1
  let value2 = map.get(key).unwrap(); // Use key to get our data back again
#+end_src

Here we are storing our data inside the map and using some unique token (the key) to keep track of our value inside the data structure when we loose access to our reference. We can do the same thing with our gc ~Arena~. We store the roots inside before we garbage collect.
#+begin_src rust
  struct Arena {
      roots: HashMap<Token, Object>,
      ...
  }
#+end_src
However we have at least two problems with this:

First, what do we use for a token? Everytime we need to root something we need a token that is unique. Even if we generated something random there is still a chance we could have two roots with the same ~Token~, which would lead to memory unsafety.

Which leads to the second problem: once something is no longer rooted, how do we remove it from the ~Arena~? We could require the user to manually call ~remove~ when they no longer need a root, but any failure to do so would result in leaking memory. That is not a good API.
**** Standing on the shoulders of boats
Thankfully, I am not the first person to think about this problem. Saoirse has a [[https://without.boats/blog/shifgrethor-iii/][blog post]] about some novel observations on rooting in Rust. This was implemented in his gc library [[https://github.com/withoutboats/shifgrethor][shifgrethor]]. I will summarize this approach below.

The first observation is that if you don't drop or move a type on the stack, then its lifetime is perfectly stack-like. Shocking I know, but the really cool part about this is that we can use it to define the way we store the roots in ~Arena~. What if instead of storing them as a map, we store them as a stack instead? When something is rooted, it is pushed on the stack. When it drops, it is popped from the stack. This also solves our problem of creating a unique ~Token~ to find our object, because when we drop we know that our item will always be the top of the root stack. So no ~Token~ needed.

In order for this to work we have to make sure the object can't move. This sounds just like the pinning! We define a new macro ~root!~ that works similarly to [[https://docs.rs/pin-utils/0.1.0/pin_utils/macro.pin_mut.html][pin_mut!]]. This ensure our objects behaves in a stack-like manner, greatly simplifying the implementation.

As far as keeping our references around post garbage collection, we know that so long as our object is rooted it will be valid. We can keep a reference into the Gc heap until we unroot (i.e. the root goes out of scope). Our ~root!~ macro will change our reference from borrowing from ~Arena~ to a borrowing from the root. So long as the root is live, our reference is valid; Even if we garbage collect.
#+begin_src rust
  let object = arena.add("new");
  // add the object to the root stack, enabling it to live past collection
  root!(object, arena);
  arena.garbage_collect(); // gc will mark the object as live
  println!("{object}"); // Object is still accessible
#+end_src
**** Returning from functions
There is one more ergonomic problem we want to solve here. Suppose we have the function below:
#+begin_src rust
  fn foo<'a>(arena: &'a mut Arena) -> Object<'a> {
      ...
      arena.add(5);
  }
#+end_src
The function takes a ~&mut Arena~, and at the end it returns an ~Object~ with the same lifetime. [[https://github.com/pretzelhammer/rust-blog/blob/master/posts/common-rust-lifetime-misconceptions.md#9-downgrading-mut-refs-to-shared-refs-is-safe][Seems fine]] right? Not so. The Rust lifetime rules [[https://doc.rust-lang.org/nomicon/lifetime-mismatch.html][require]] that the /mutable borrow/ of ~Arena~ now lasts for the lifetime ~'a~! This means we can't reuse ~Arena~ while the ~Object~ is borrowed from it. We could just ~root!~ the object, but that adds overhead to every call. In my interpreter, nearly every method takes ~&mut Arena~, so that would get expensive fast.

To work around this we create a new macro ~rebind!~
#+begin_src rust
  rebind!(object, arena);
#+end_src
This macro releases the /mutable/ borrow and reborrows the object with an /immutable/ borrow. This frees ~Arena~ to be used by other code while still being [[https://github.com/CeleritasCelery/rune/issues/2][sound]].
**** Preventing escapes
This approach works fine for rooting a single object, but what if we have a whole collection of objects? You might be tempted to think that would be an non-issue, but consider the problem below:
#+begin_src rust
  let rooted: &mut Vec<Object<'root>> = ...; // we have a vec of root objects
  let object: Object<'arena> = arena.add("new"); // object is bound to arena
  rooted.push(object); // object is now bound to rooted
  arena.garbage_collect(); // Object is marked as live and not freed

  // Object is no longer rooted, but still bound to the root lifetime
  let escape: Object<'root> = rooted.pop().unwrap();
  arena.garbage_collect(); // Object is freed
  println!("{escape}"); // Oh No! Use after Free!
#+end_src
We cannot move references out without some way of making sure they stay rooted. Thankfully shifgrethor comes to the rescue here again with it's ~Gc~ type.

Once again we can model after the ~pin~ API, since we are trying to solve a similar problem. If you have a ~Pin<P>~ you know that the data point to by ~P~ will not move. Similarly, we create a ~Root<T>~ type that guarantees ~T~ will not move *and* it's rooted. We use the ~struct_root!~ macro to take a data structure ~T~ and returns a ~&mut Root<T>~ to it.
#+begin_src rust
  let arean: Arena = ...;
  struct_root!(my_struct, arena);
  let _: &mut Root<Vec<Object>> = my_struct;
  // get a reference to vec from root
  let len = my_struct.len();
  // use a special function to mutate
  my_struct.root_push(object);
  // use projection
  let slice: &[Root<Object>] = my_struct.as_slice();
  // Object with lifetime bound to root
  let object: Object<'_> = slice[0].as_obj();
#+end_src

With this API, we can safely get a ~&T~ out when we need to. But mutating the ~T~ requires unsafe methods (like [[https://doc.rust-lang.org/std/pin/struct.Pin.html#method.map_unchecked_mut][map_unchecked_mut]]) to ensure we don't expose roots as in the example above. Using a similar approach to [[https://doc.rust-lang.org/std/pin/index.html#projections-and-structural-pinning][pin projection]] you can get a ~Root~ to the fields of rooted struct. For example if you have a ~&Root<(T, U)>~ it is safe get a ~&Root<T>~ or ~&Root<U>~. For some the std lib types (vec, hashmap, option, etc) I have already implemented some safe mutation methods like ~push~. If you have a structure that is just built out of these stdlib data structures, you could use a proc macro to derive the "root projection" methods for it.

*** The problem with aliasing
There is still one subtle problem here. You see, we now have a ~&mut Root<T>~, and when we garbage collect, we will trace through ~T~ with ~&T~. However ~&mut T~ guarantees that that it is unique. To break this invariant means undefined behavior. Shifgrethor does not handle this, instead requiring that all roots be immutable (even forbidding interior mutability). Ugh.

How about we use ~UnsafeCell~? It is full of dark magic that lets us do thing we couldn't normally do.

 /* reads documentation */
#+begin_quote
Note that only the immutability guarantee for shared references is affected by UnsafeCell. The uniqueness guarantee for mutable references is unaffected. There is no legal way to obtain aliasing &mut, not even with UnsafeCell<T>.
#+end_quote
Oh, biscuits. What other options do we have? I am sure Rust has an ~AliasCell~ that let's us work around this, right?

 /* googles frantically */

Nope. Though apparently we not the only ones who need this. The std lib created a [[https://github.com/rust-lang/rust/pull/82834][hack]] to avoid miscompilations with aliasing mutable references that is used in [[https://github.com/tokio-rs/tokio/pull/3654][Tokio]] as well. We could take that route (and I was really tempted to) but let's see if there is another way we could fix this.
**** Can we approach the problem from the other side?
Actually there is a way ~UnsafeCell~ can help us here. There is one legal way in which you can have aliasing ~&mut T~. By design, a ~&mut T~ can alias with a ~&UnsafeCell<T>~, (but not the other way around). So long as we don't have any ~&T~ at the same time; this is sound. But this of course means interior mutability. We could try just putting ~RefCell~'s everywhere, but that means we are going have to debug runtime panics instead of compile time errors. We /really/ don't want that.
**** Qcell to the rescue
[[https://docs.rs/qcell/latest/qcell/index.html][Qcell]] is a crate trying to design a compile time ~RefCell~. It makes a bunch of different cell types, each with their own set of trade off, that give you exactly that. We are going to use [[https://docs.rs/qcell/latest/qcell/struct.LCell.html][LCell]], which is zero cost and perfect for our use case. With this type, multiple cells have a shared owner that control when a cell can be borrowed mutable or immutable. To make this safe we define the following conditions:

1. We can borrow a ~Root~ as immutable if we have a ~&RootOwner~.
2. We can borrow a ~Root~ as mutable if we have a ~&mut RootOwner~ *and* we have a ~&Arena~. This ensures that we can never call garbage collect while our mutable reference is live, because garbage collect requires a mutable borrow of ~Arena~!
#+begin_src rust
  impl<'id, T> Root<'id, T> {
      pub(crate) fn borrow<'a>(
          &'a self,
          owner: &'a RootOwner<'id>
      ) -> &'a RootRef<T> {...}

      pub(crate) fn borrow_mut<'a>(
          &'a self,
          owner: &'a mut RootOwner<'id>,
          _: &'a Arena
      ) -> &'a mut RootRef<T> {...}
  }
#+end_src
Using compile-time interior mutability makes our code more verbose then it needs to be, but that is the price we pay sometimes for correctness. I would love for Rust to get an ~AliasCell~ that solves this problem for everyone. Either way, this is what the final API looks like:
#+begin_src rust
  let root_owner: RootOwner<'id> = ...;
  let arean: Arena = ...;
  struct_root!(my_struct, arena);
  let _: &Root<'id, Vec<Object>> = my_struct;
  // immutable borrow
  let len = my_struct.borrow(&root_owner).len();
  // mutable borrow
  my_struct.borrow_mut(&mut root_owner, &arena).push(object);
#+end_src

*** A Safe GC
So there you have it! A safe, precise, garbage collector in stable Rust! Now, this comes with a few caveats. It is often said that solving a general problem is three times harder then solving a specific problem. I am solving the specific problem here; creating a GC for my VM. This not ready to ship as a general purpose library without more work. But I am confident it could be made into a library if needed. Right now the garbage collector is about as naive as possible. But future changes will be under-the-hood improvements that don't change the API.

What I think is really cool is that the API is *safe*! You can't create this in C or C++; The type system is not powerful enough. Rust enables us to have "fearless garbage collection", and no longer be scared of the "nasty bugs" that we might create. As an anecdote, I was pleasantly surprised to find that when I turned on reclaiming memory in my gc, everything just worked first time; No memory leaks, no use-after-free. The API just took care of it at compile time. Miri was satisfied as well.

Overall, I am pretty happy with how it turned out. That being said, there is *a lot* of unsafe code behind the scenes. I am the only person that has reviewed it, and I am not smart enough to get everything right. I created a [[https://github.com/CeleritasCelery/rune/labels/unsound%3F][unsound?]] Label on Github that tracks some of the code I have the least confidence in. If you are initiated in the dark arts of the [[https://doc.rust-lang.org/nomicon/][nomicon]], I would love for you to [[https://github.com/CeleritasCelery/rune/issues?q=is%3Aissue+label%3Aunsound%3F+][prove me wrong]].

I am going to continue work on bootstrapping more elisp files to eventually bootstrap the elisp byte compiler and use that to test my VM. I was forced to take break from that effort and implement the garbage collector because I kept using too much memory! Implementing the garbage collector was a much bigger rabbit hole than I expected. Hopefully this will help move the community forward on the quest for a Rust GC.

****  Have a comment?
View the discussion on [[https://www.reddit.com/r/rust/comments/u21w97/implementing_a_safe_garbage_collector_in_rust/][Reddit]] or [[https://news.ycombinator.com/item?id=31166368][Hacker News]], send me an [[mailto:troy.hinckley@dabrev.com][email]], or open an [[https://github.com/CeleritasCelery/rune/issues/new][issue]].

[fn:5] Why is reference counting slower then garbage collection? There is a lot that goes into it, but it boils down to two main issues:
1. Every time you copy a ~Rc~ pointer you need to update the reference count. This involves reading the memory, updating the count, and writing it back. Compare that to an "normal" pointer copy where you don't need to even /access/ the memory at all. GC's do have to trace the memory eventually, but this overhead can be moved to a time when it will have less impact (or even moved to another thread). RC overhead needs to happen real time, and happens /every time/.
2. RC can fall victim to "destructor avalanche" when the root of a chain of objects goes out of scope.  This results in unbounded pause times. Modern GC's by contrast are usually incremental, and will do work in small chunks to preventing long pauses.

With all of these issues, there are techniques to try and mitigate them and get some performance back. But even a naive GC can often beat a well optimized RC implementation. And optimized GC (like JVM or V8) will always outclass reference counting. See [[https://softwareengineering.stackexchange.com/questions/30254/why-garbage-collection-if-smart-pointers-are-there][this SO post]] and [[https://web.archive.org/web/20200325094430/http://flyingfrogblog.blogspot.com/2010/12/why-gc-when-you-have-reference-counted.html][follow up post]] for more.
[fn:6] I don't think this is true in Rust though. My best guess is that scanning the stack would violate some of rust's aliasing rules and be UB.

** DONE [#B] A vision of a multi-threaded Emacs
:PROPERTIES:
:EXPORT_DATE: 2022-05-19
:EXPORT_FILE_NAME: a-vision-of-a-multi-threaded-emacs
:END:
*** The Threading library
Starting in Emacs 26 some very ambitious changes were added. Basic thread support was enabled, laying the groundwork for a future concurrent emacs. The [[https://www.gnu.org/software/emacs/manual/html_node/elisp/Threads.html][docs]] layout this possibility:

#+begin_quote
Emacs Lisp provides a limited form of concurrency, called threads. All the threads in a given instance of Emacs share the same memory. Concurrency in Emacs Lisp is “mostly cooperative”, meaning that Emacs will only switch execution between threads at well-defined times. However, the Emacs thread support has been designed in a way to later allow more fine-grained concurrency
#+end_quote

What would a future with fine-grained concurrency look like? Could we have an Emacs that uses more then 1 thread? This post sketches out some rough ideas of what that could look like from an elisp perspective. I am going to take the easy way out and completely ignore  /how/ to actually implement this, just speculating on the big what-ifs.

*** Concurrency vs parallelism
The ~thread~ feature is specifically trying to enable /concurrency/, which is the ability to interweave lines of execution to make progress on more then one program at a time. This is the model used by async/await and coroutines. Concurrency is useful when your application is IO bound. The library enables you to switch between concurrent programs at designated points, which is why it is call /cooperative/ concurrency. /Parallelism/ on the other hand is the ability to actually run multiple programs at the same time. This is useful when your application is CPU bound. See [[https://oxylabs.io/blog/concurrency-vs-parallelism][this post]] for a more detailed explanation.

There are two main scenarios where concurrency/parallelism are useful.
1. When I want some elisp task done in the background without blocking my main user thread, I can use concurrency. This includes things like handling filter output, indexing buffers, watching for changes, etc. The background task will "steal" idle time from my main thread to make incremental progress on its job.
2. When I want to get the results of some task faster, I need parallelism. This includes things like updating or searching buffers, applying font lock, or loading code. In order to do these task faster I need multiple threads running at the same time.

Note that parallelism can service both use case 1 and 2, but concurrency can only deal with use case 1. In some sense, parallelism is a superset of concurrency. All parallel code is concurrent, but concurrent code is not necessarily parallel. For this reason, I am much more interested in a parallel Emacs then just a concurrent one.

*** What level of parallelism do you want?
My oversimplification of parallel languages breaks them into three categories:

- Level 1 - memory unsafe and data races allowed  :: Languages where incorrect code can lead to corruption of the program state and segfaults. This includes C++ and Swift.
- Level 2 - memory safe and data races allowed :: Languages where parallelism is memory safe, but can still lead to data races. This includes Java and Go.
- Level 3 - memory safe and no data races :: Languages that enable [[https://doc.rust-lang.org/book/ch16-00-concurrency.html][fearless concurrency]] by eliminating unguarded access to shared-memory. This includes Clojure, Rust, and TCL.

Generally the closer you are Level 1 the more footguns there are, but the more performance you can squeeze out. The higher you go the easier concurrent code is to write, but you have less performance and control. The exception to this is Rust, which is a safe Level 3 language with the performance of a Level 1.

So where do we want Emacs to land on this spectrum? The creator of [[https://github.com/colesbury/nogil][nogil]] python, a multi-threaded python implementation, [[https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0/edit][said this]]:

#+begin_quote
The project aims for a concurrency model that matches the threads + shared memory model implemented in common operating systems... The shared-memory model is notoriously difficult to program correctly, but provides a good base to build better abstractions because it closely matches the primitives provided by the operating system (e.g. threads, mutexes).
#+end_quote

The argument is that Level 2 is the right balance, because you avoid crazy bugs you get with unsafe languages but still have more flexibility then level 3. You should just give programmers the tools they need to build safe abstractions.

I, however, disagree with that take. As the author said, shared memory is "notoriously difficult" to do correctly. As Emacs pulls in hundreds of packages, the potential for data races grows exponentially. Even Emacs current threads library [[https://nullprogram.com/blog/2018/05/31/][suffers from data races]] which is one of the reason I believe it has not seen much adoption. We need to make concurrency as pain free as possible if it going to be usable. Therefore I am in the "Emacs parallelism should be level 3" boat.

**** What are some requirements for parallel Emacs
So what are the standards we want for a multi-threaded Emacs implementation? Here is my short list:

1. No [[https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/][function coloring]] or special requirements on functions. One of Emacs' big advantages is the huge bulk of existing lisp code. We want to reuse as much as we can. This is generally only a problem with concurrent schemes like async/wait.
2. No data races. This will make program significantly easier to write correctly, but also is going to make our code more limited (** /foreshadowing **/).
3. We want the behavior of multi-threaded code to be as close to single-threaded code as possible. More on this later.

*** A jumping off point
Most people have never heard of TCL (or if they have they've never used it) but I find it has a [[https://www.activestate.com/blog/threads-done-right-tcl/][very simple approach]] to multi-threading. Essentially the an interpreter can work in its own thread, and carries with it all of its state. This is the multi-interpreter approach; Every thread starts in a clean environment with its own interpreter. "Messages" can be passed to any thread and they can return a result. In elisp it could look something like this.

#+begin_src emacs-lisp
  (let ((thread1 (make-thread))
        (thread2 (make-thread))
        (filename "file.txt")
        result1 result2)
    ;; Send commands to the threads
    (thread-send thread1 #'setup-function)
    (setq result1 (thread-send thread1 #'my-function))
    ;; closure captures a copy of the variable
    (setq result2 (thread-send thread2 (lambda () (delete-file filename))))
    ;; run some other code while threads are working
    (my-other-function)
    ;; wait for the results to populate
    (thread-wait result1 result2)
    ;; return the results in list
    (list result1 result2))
#+end_src

This is really simple and really effective, but it has some limitations. First is that since each thread starts a new interpreter, you need to load a bunch of lisp code to do almost anything useful. This means that thread overhead is significant and is not a good fit for small tasks. Second, since you are copying objects between threads when you pass them with ~thread-send~ you can't modifying existing buffers[fn:3]. And buffers are probably the most important use case here. Let's see if we can fix that.

*** Can you pass the buffer please?
What if instead of needing to copy the buffers between threads, they could be shared? I know, I know, shared-memory is a footgun, but we are going to use mutex's! So it more like sharing in pre-school where everyone get's a turn. Each buffer is guarded by a mutex, and only one thread can have access to a buffer at a given time. The way you acquire the mutex is by switching to that buffer (using ~set-buffer~ , ~switch-to-buffer~, or ~with-current-buffer~). Just as you can only have a single "current buffer", you can only have the mutex for a single buffer at a time. A thread can switch to a buffer, do some operations, then release it. This is all well and good, but we have a major issue; shared-state.

You see, for a buffer to really be useful you need have the buffer local variables. Without those you can't even know the ~major-mode~! But buffer local variable can share data with global variables, and each thread has its own set of globals. Consider the code below:
#+begin_src emacs-lisp
  (defvar local nil)
  (defvar global nil)
  (make-variable-buffer-local 'local)
  (setq local '(1 2 3))
  (setq global local)
#+end_src

Here both ~local~ and ~global~ share the same cons cells. If I mutate one it will mutate the other. This obviously won't work if I am sharing buffer local variables between threads. What we need to sever the ties between these. Buffer local variables can't share any data with non-buffer local variables. You could setup a write barrier that would make copies when a thread releases the mutex. But I am not going to get into the /how/ (I get to ignore implementation details remember?).

This would technically make multi-threaded emacs have different behavior then the old single threaded one (I told you we would talk about that later). But I would argue that if you are relying on sharing data between globals and buffer-locals for the correct operation of your code, it is in serious need of a refactor. I imagine that in real life this situation is very rare.

*** Cutting down initialization
As it currently stands you basically need to load your entire init file in each new thread. Why can't you just load the bare minimum elisp? Consider what would happen when a buffer local hook calls some function from another package? We need to make sure the code is loaded, and the only way we can do that is by loading the init file which contains all package initialization. There is also the problem that the state can get out sync. Each thread would start in a clean state, but this is not going to match the state of your main thread. This impedance mismatch is a clear source of bugs.

What if we did something crazy? What if we said that all functions are shared between threads? If you think about it, this is almost a perfect match. Function rarely change, and when they do, you can just replace the whole function atomically. However to do this you would need to address the /functional literal mutation problem/ I talk about [[https://coredumped.dev/2021/04/07/when-pure-function-lie/][here]]. Otherwise you could have multiple threads mutating the same function constants and potentially corrupt the VM state. But again, /implementation details/.

Okay so that takes care of functions, but what about variables? Sharing variables would lead to data races, which is exactly what we are trying avoid. What if instead of sharing, we copied variables on demand? Hear me out! The first time a "sub-thread" does a lookup of a global variable, its value is copied from the main thread. This sets the initial value in that thread. From then on that copy of the value is "owned" by the thread and it can be mutated or read whenever. This would also help with the out-of-sync with the main thread problem we mentioned earlier. The state in your sub-thread would be much closer to the main global state. There could be an message queue internal to the VM that sends these requests back and forth. At certain points, the main thread would check the queue and send the values.

But this also means that sub-threads could spend a decent amount of time waiting for the main thread to be ready to send the values it needs, at least at the start. There could be a couple ways to alleviate this. The one that comes to mind is you could cache the list of variables used at the call site of the thread creation. Then next time the thread is called you eagerly copy all the variables it has used before. This would make repeated initialization of the same thread much faster, but could also mean you get different variables the first time vs following times (if the calling function changed something immediately afterwards for example). As with everything; trade-offs.

There is one big reason for all this song and dance around buffers locals, functions, and variables: reusing existing elisp code. Languages that were created with concurrency in mind have designed their languages around these considerations. But Emacs is a giant ball of mutable state. Taming that to do something useful in a multi-threaded world while still reusing existing code is tricky.
*** Going Green
So our threads are pretty cheap to create (as threads go), but not to keep around. Each elisp thread maps to a OS thread, and even when thread is idle it is still taking OS resources. Go and Clojure have solved this by creating so called green threads that are managed by the runtime. The green threads will be executed on OS threads, but they can be created, destroyed and managed by the VM. In Go you can create thousands of green threads and it will not impact the system. Don't try at home with OS threads.

Now that we have green threads, the observant among you will notice that we have basically reinvented goroutines. All we need to do is add channels and we can have something close to [[https://clojuredocs.org/clojure.core.async][core.async]]. I imagine usage looking something like this:

#+begin_src emacs-lisp
  ;; Delete 3 files concurrently
  (let ((files '("file1.txt" "file2.txt" "file3.txt")))
    (dolist (file files)
      (go (delete-file file))))

  ;; loop through all buffers and insert "a" for some reason...
  (let* ((c (chan)))
    (go (loop-chan chan buffer
                   (with-current-buffer buffer
                     (insert "a"))))
    (dolist (buffer (buffer-list))
      (chan-put c buffer)))

  ;; run a buffer search on another thread and yield the output
  (let ((queries (chan))
        (results (chan)))
    (go (let* ((message (channel-take queries))
               (buffer (car message))
               (string (cdr message)))
          (chan-put results
                    (with-current-buffer buffer
                      (save-excursion
                        (re-search-forward string)
                        (buffer-substring
                         (point)
                         (+ 5 (point))))))))
    (chan-put queries (cons other-buffer "foo"))
    (do-something-else)
    ;; wait till the routine returns
    (setq my-string (chan-take results)))
#+end_src

*** So where does that leave us?
We have created some simple light weight green threads for Emacs. Well, we didn't actually create anything, but we sure did talk about it a lot! The things that I like about his approach is that threads are easy to create and use to accomplish work in parallel. There are no data races and footguns have been minimized. But I can still see a few open problems that are not solved:

1. The most important buffer in Emacs is the one you are currently editing. But with the mutex scheme, you can't use any other threads to work on that buffer! If you want to index or search or syntax-highlight the buffer, that still needs to use your main thread, meaning that the user is blocked. I don't like it, but not sure of a clean way to fix it.
2. What about when you need to iterate over all buffers (like with ~ibuffer~)? Here you would need to acquire the mutex for each one in turn. If another thread is using a buffer the main thread will have to wait. Hopefully sub-threads would choose to do the work incrementally, giving time for the thread to yield the mutex.
3. Since the sub-threads take their global variables from the main thread, You can't load code in parallel. Only the main thread can load code that can be used by everyone.
4. I haven't even mentioned a bunch of other multi-threading concerns like cancellation, atomics, garbage collection, message queue buffering, semaphores, signals, error reporting, debugging, concurrent data structures, C integration, deadlock, and livelock to name a few. Perhaps those are a topic for a future post.

It has been fun to speculation about multi-threaded Emacs. But the real question is, would it be worth it? Emacs has gotten along just fine with a single thread; In fact many (most?) dynamic languages have. I would guess that threads would only be useful in about 10% of the programming tasks you would do in elisp. But when threads can be used, they would be big boon.

As with everything in engineering, concurrency comes with trade-offs. Implementing a scheme like I described would be a monumental task. It would probably involve a complete rewrite of the core runtime[fn:1]. Also anytime you make an interpreter multi-threaded, you make single threaded code slower[fn:2]. There is no avoiding that. If 90% of code is still single-threaded, is that worth the cost?

Anyways, I would love to get some feedback on the ideas presented. Are there obvious holes that I missed? Would this scheme be useful? Do you know way that these could be implemented (or would not be possible to implement)? How does this compare to other dynamic languages? Do you prefer the more "thread-like" or "green-thread-like" approach? Is there a way to address some of the problems presented above?
**** have a comment?
View the discussion on [[https://www.reddit.com/r/emacs/comments/utzxir/a_vision_of_a_multithreaded_emacs/][Reddit]], [[https://news.ycombinator.com/item?id=31559818][Hacker News]], or send me an [[mailto:troy.hinckley@dabrev.com][email]]

[fn:1] Similar to what is being attempted for python [[https://github.com/colesbury/nogil][here]] and [[https://github.com/larryhastings/gilectoy][here]].

[fn:2] Why does adding multi-threading make single-threaded code slower? It essentially comes down the fact that synchronizing memory between cores is [[https://spcl.inf.ethz.ch/Publications/.pdf/atomic-bench.pdf][slow]]. So unless your interpreters are completely independent (which would not be very useful), you are going to add some overhead. Now this doesn't have to be a lot, it can be just a couple percentage points. But it will always be something. The [[https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0/edit][nogil design doc]] goes into great detail about some of performance implications (for python) and how to mitigate them. See also this [[https://github.com/ocamllabs/compiler-hacking/wiki/OCaml-Multicore-Runtime][design doc]] for multi-core OCaml or [[https://www.atdot.net/~ko1/activities/2016_rubykaigi.pdf][this presentation]] on the Actor model in Ruby.

[fn:3] And you can't share anything that doesn't have a clear [[https://www.gnu.org/software/emacs/manual/html_node/elisp/Streams-Intro.html][readable]] representation such as windows, frames, or makers.
